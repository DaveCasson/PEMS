{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undercatch Correction ##\n",
    "\n",
    "Implement undercatch corrections to station data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input file locations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "from geopy.distance import geodesic\n",
    "from scipy.spatial import KDTree\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from undercatch_processing import *\n",
    "\n",
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input file locations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Paths\n",
    "input_path = '/Users/dcasson/Data/pems/'\n",
    "input_station_data_path = Path(input_path,'station_data')\n",
    "\n",
    "#Input station data\n",
    "cdn_complete_stations = Path(input_station_data_path,'Station Inventory EN.csv')\n",
    "gsod_stations = Path(input_station_data_path,'isd-history.csv')\n",
    "ghcnd_stations = Path(input_station_data_path,'ghcnd-stations.csv')\n",
    "\n",
    "# Input SC-Earth data\n",
    "undercatch_stations_nc = Path(input_path,'sc_earth','undercatch_stations.nc')\n",
    "reanalysis_data_path = Path(input_path,'era5')\n",
    "\n",
    "# Smith, 2019 data\n",
    "smith_2019_stations = Path(input_station_data_path,'EN_StationCatalogue_2019.csv')\n",
    "smith_2019_hourly_path = Path(input_path,'smith_2019/hourly_data/')\n",
    "smith_2019_daily_path = Path(input_path,'smith_2019/daily_data/')\n",
    "\n",
    "output_path = '/Users/dcasson/Data/pems/undercatch/'\n",
    "sc_earth_path = Path(output_path,'sc_earth')\n",
    "interim_station_path = Path(output_path,'interim_stations')\n",
    "reanalysis_station_path = Path(output_path,'reanalysis_stations')\n",
    "merged_station_path = Path(output_path,'merged_stations')\n",
    "undercatch_station_path = Path(output_path,'undercatch_stations_test')\n",
    "plot_path = Path(output_path,'plots')\n",
    "\n",
    "\n",
    "first_filter_year = 2004\n",
    "reanalysis_variable = 'windspd'\n",
    "\n",
    "#Output files\n",
    "cdn_year_filtered_stations = Path(output_path,'cdn_station_inventory.csv')\n",
    "undercatch_stations_csv = Path(output_path,'undercatch_from_nc.csv')\n",
    "undercatch_stations_update_csv = Path(output_path,'undercatch_stations_update.csv')\n",
    "evaluation_stations_csv = Path(output_path,'evaluation_stations.csv')\n",
    "\n",
    "# Make output directories\n",
    "os.makedirs(smith_2019_daily_path, exist_ok=True)\n",
    "os.makedirs(interim_station_path, exist_ok=True)\n",
    "os.makedirs(reanalysis_station_path, exist_ok=True)\n",
    "os.makedirs(merged_station_path, exist_ok=True)\n",
    "os.makedirs(sc_earth_path, exist_ok=True)\n",
    "os.makedirs(undercatch_station_path, exist_ok=True)\n",
    "os.makedirs(plot_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Canadian Large Dataset, to remove old stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered stations saved to /Users/dcasson/Data/pems/undercatch/cdn_station_inventory.csv\n"
     ]
    }
   ],
   "source": [
    "def filter_stations_by_year(input_csv_path, output_csv_path, year):\n",
    "    # Load the CSV file with appropriate delimiter and skip initial lines\n",
    "    stations_df = pd.read_csv(input_csv_path, delimiter=',', skiprows=3)\n",
    "    \n",
    "    # Filter out rows where 'DLY Last Year' is before the specified year\n",
    "    filtered_df = stations_df[stations_df['DLY Last Year'] >= year]\n",
    "    \n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    filtered_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Filtered stations saved to {output_csv_path}\")\n",
    "\n",
    "filter_stations_by_year(cdn_complete_stations, cdn_year_filtered_stations, first_filter_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read stations from SC-Earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undercatch stations saved to /Users/dcasson/Data/pems/undercatch/undercatch_from_nc.csv\n"
     ]
    }
   ],
   "source": [
    "def read_undercatch_stations(nc_file_path, output_csv_path):\n",
    "    # Open the netCDF file using xarray\n",
    "    ds = xr.open_dataset(nc_file_path)\n",
    "    \n",
    "    # Extract variables\n",
    "    station_ids = ds['station_ID'].values\n",
    "    latitudes = ds['latitude'].values\n",
    "    longitudes = ds['longitude'].values\n",
    "    elevations = ds['elevation'].values\n",
    "    \n",
    "    # Process station_IDs to remove prefix\n",
    "    processed_station_ids = [sid.split('_')[1][:5] for sid in station_ids]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'full_station_ID':station_ids,\n",
    "        'station_ID': processed_station_ids,\n",
    "        'latitude': latitudes,\n",
    "        'longitude': longitudes,\n",
    "        'elevation': elevations\n",
    "    })\n",
    "    \n",
    "    # Output to CSV\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Undercatch stations saved to {output_csv_path}\")\n",
    "\n",
    "read_undercatch_stations(undercatch_stations_nc, undercatch_stations_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge station metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined /Users/dcasson/Data/gpep/tuolumne/data_prep/stations/prcp_subset.csv and /Users/dcasson/Data/pems/station_data/ghcnd-stations.csv saved to /Users/dcasson/Data/gpep/tuolumne/data_prep/stations/prcp_subset_ghcnd.csv\n",
      "Joined /Users/dcasson/Data/gpep/tuolumne/data_prep/stations/prcp_subset.csv and /Users/dcasson/Data/pems/station_data/isd-history.csv saved to /Users/dcasson/Data/gpep/tuolumne/data_prep/stations/prcp_subset_gsod.csv\n"
     ]
    }
   ],
   "source": [
    "def join_datasets(file1_path, file2_path, join_column_file1, join_column_file2, output_path, join_type='inner'):\n",
    "    # Load the datasets\n",
    "    df1 = pd.read_csv(file1_path)\n",
    "    df2 = pd.read_csv(file2_path)\n",
    "\n",
    "    # Ensure the columns for joining are strings and strip trailing .0 if present\n",
    "    df1[join_column_file1] = df1[join_column_file1].astype(str).str.rstrip('.0')\n",
    "    df2[join_column_file2] = df2[join_column_file2].astype(str).str.rstrip('.0')\n",
    "\n",
    "    # Perform the join\n",
    "    merged_df = pd.merge(df1, df2, left_on=join_column_file1, right_on=join_column_file2, how=join_type)\n",
    "\n",
    "    # Save the result to CSV\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"Joined {file1_path} and {file2_path} saved to {output_path}\")\n",
    "\n",
    "#join_datasets(undercatch_stations_csv, cdn_year_filtered_stations, 'station_ID', 'WMO ID', undercatch_stations_update_csv)\n",
    "\n",
    "#join_datasets(undercatch_stations_update_csv,smith_2019_stations,'Climate ID', 'StationID', evaluation_stations_csv)\n",
    "\n",
    "tuolumne_prcp = '/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/prcp_subset.csv'\n",
    "ghcnd = '/Users/dcasson/Data/pems/station_data/ghcnd-stations.csv'\n",
    "gsod = '/Users/dcasson/Data/pems/station_data/isd-history.csv'\n",
    "\n",
    "join_datasets(tuolumne_prcp,ghcnd,'station_ID','StationID','/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/prcp_subset_ghcnd.csv')\n",
    "join_datasets(tuolumne_prcp,gsod,'station_ID','USAF','/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/prcp_subset_gsod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching complete. Results saved to matched_stations.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load GHCNd stations metadata\n",
    "ghcnd_stations = pd.read_csv('/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/tuolumne_stations_ghcnd.csv')  # Adjust the path as needed\n",
    "\n",
    "# Load CDEC stations metadata\n",
    "cdec_stations = pd.read_csv('/Users/dcasson/Data/pems/station_data/CDEC.csv')  # Adjust the path as needed\n",
    "\n",
    "# Function to calculate distance between two coordinates\n",
    "def calculate_distance(coord1, coord2):\n",
    "    return geodesic(coord1, coord2).km\n",
    "\n",
    "# Create an empty list to store matches\n",
    "matches = []\n",
    "\n",
    "# Iterate through each GHCNd station\n",
    "for idx, ghcnd_station in ghcnd_stations.iterrows():\n",
    "    ghcnd_coords = (ghcnd_station['latitude'], ghcnd_station['longitude'])\n",
    "    closest_station = None\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    # Iterate through each CDEC station\n",
    "    for _, cdec_station in cdec_stations.iterrows():\n",
    "        cdec_coords = (cdec_station['Latitude'], cdec_station['Longitude'])\n",
    "        distance = calculate_distance(ghcnd_coords, cdec_coords)\n",
    "\n",
    "        # Check if the current CDEC station is the closest match\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_station = cdec_station\n",
    "\n",
    "    # If the closest station is within a reasonable distance, consider it a match\n",
    "    if min_distance < 3.0:  # Adjust the distance threshold as needed\n",
    "        matches.append({\n",
    "            'ghcnd_id': ghcnd_station['station_ID'],\n",
    "            'ghcnd_name': ghcnd_station['StationName'],\n",
    "            'cdec_id': closest_station['ID'],\n",
    "            'cdec_name': closest_station['Station Name'],\n",
    "            'cdec_operator': closest_station['Operator'],\n",
    "            'distance_km': min_distance\n",
    "        })\n",
    "\n",
    "# Convert matches to a DataFrame for easy analysis\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "# Save the matches to a CSV file\n",
    "matches_df.to_csv('/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/matched_stations.csv', index=False)\n",
    "\n",
    "print('Matching complete. Results saved to matched_stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3020610.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3012050.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3050519.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3031480.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3050778.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3051R4R.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3026KNQ.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3024925.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3025297.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3011892.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3053536.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3034795.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3030720.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3036205.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3023200.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3023740.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3035208.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3031092.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3031093.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_1176755.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3017282.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3015523.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_sc_earth_station_data(nc_file_path, csv_file_path, output_dir):\n",
    "    # Load the netCDF file\n",
    "    ds = xr.open_dataset(nc_file_path)\n",
    "    \n",
    "    # Load the CSV file\n",
    "    stations_df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a dictionary to map station_ID to station_number\n",
    "    station_id_map = {str(ds['station_ID'][i].values): i for i in range(len(ds['station_ID']))}\n",
    "    \n",
    "    # Iterate through the stations in the CSV file\n",
    "    for _, row in stations_df.iterrows():\n",
    "        climate_id = row['Climate ID']\n",
    "        if not pd.isna(climate_id):\n",
    "            station_id = row['full_station_ID']\n",
    "            station_number = station_id_map.get(station_id, None)\n",
    "            \n",
    "            if station_number is not None:\n",
    "                # Extract data for the station\n",
    "                time = ds['time'].values\n",
    "                prcp = ds['prcp'].isel(station_number=station_number).values\n",
    "                tmean = ds['tmean'].isel(station_number=station_number).values\n",
    "                wind = ds['wind'].isel(station_number=station_number).values\n",
    "                \n",
    "                # Create a DataFrame for the station data\n",
    "                station_data = pd.DataFrame({\n",
    "                    'time': time,\n",
    "                    'prcp': prcp,\n",
    "                    'tmean': tmean,\n",
    "                    'wind': wind\n",
    "                })\n",
    "                \n",
    "                # Define the output file path\n",
    "                output_file_path = os.path.join(output_dir, f'station_{str(climate_id)}.csv')\n",
    "                \n",
    "                # Save the DataFrame to a CSV file\n",
    "                station_data.to_csv(output_file_path, index=False)\n",
    "                print(f\"Station data saved to {output_file_path}\")\n",
    "\n",
    "extract_sc_earth_station_data(undercatch_stations_nc, undercatch_stations_update_csv, sc_earth_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract reanalysis data for undercatch stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing station 3020610\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (25,) (41,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m         process_reanalysis_station(ds, station_id, station_name, orig_lat, orig_lon, variable, output_dir)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Execute extraction \u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mextract_reanalysis_station_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mundercatch_stations_update_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreanalysis_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreanalysis_station_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreanalysis_variable\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 76\u001b[0m, in \u001b[0;36mextract_reanalysis_station_data\u001b[0;34m(csv_path, nc_dir, output_dir, variable)\u001b[0m\n\u001b[1;32m     74\u001b[0m orig_lon \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing station \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m \u001b[43mprocess_reanalysis_station\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_lat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_lon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mprocess_reanalysis_station\u001b[0;34m(ds, station_id, station_name, orig_lat, orig_lon, variable, output_dir)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Find the nearest grid point using the lat-lon variables in the dataset\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m nearest_lat_idx, nearest_lon_idx \u001b[38;5;241m=\u001b[39m \u001b[43mfind_nearest_grid_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_lat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_lon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Extract the data for the nearest grid point\u001b[39;00m\n\u001b[1;32m     28\u001b[0m point_data \u001b[38;5;241m=\u001b[39m ds[variable]\u001b[38;5;241m.\u001b[39misel(rlat\u001b[38;5;241m=\u001b[39mnearest_lat_idx, rlon\u001b[38;5;241m=\u001b[39mnearest_lon_idx)\u001b[38;5;241m.\u001b[39mto_dataframe()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mfind_nearest_grid_point\u001b[0;34m(ds, orig_lat, orig_lon)\u001b[0m\n\u001b[1;32m     12\u001b[0m abs_diff_lat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(latitudes \u001b[38;5;241m-\u001b[39m orig_lat)\n\u001b[1;32m     13\u001b[0m abs_diff_lon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(longitudes \u001b[38;5;241m-\u001b[39m orig_lon)\n\u001b[0;32m---> 14\u001b[0m combined_diff \u001b[38;5;241m=\u001b[39m \u001b[43mabs_diff_lat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mabs_diff_lon\u001b[49m\n\u001b[1;32m     15\u001b[0m min_diff_idx \u001b[38;5;241m=\u001b[39m combined_diff\u001b[38;5;241m.\u001b[39margmin()\n\u001b[1;32m     16\u001b[0m nearest_lat_idx, nearest_lon_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munravel_index(min_diff_idx, latitudes\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (25,) (41,) "
     ]
    }
   ],
   "source": [
    "def load_points(csv_path):\n",
    "    \"\"\"Load points from CSV file.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Convert station_ID to string without tuple notation\n",
    "    df['station_ID'] = df['station_ID'].apply(lambda x: str(x).strip(\"(),'\"))\n",
    "    return df\n",
    "\n",
    "def find_nearest_grid_point(ds, orig_lat, orig_lon):\n",
    "    \"\"\"Find the nearest grid point in the dataset for the given latitude and longitude.\"\"\"\n",
    "    latitudes = ds['latitude'].values\n",
    "    longitudes = ds['longitude'].values\n",
    "    abs_diff_lat = abs(latitudes - orig_lat)\n",
    "    abs_diff_lon = abs(longitudes - orig_lon)\n",
    "    combined_diff = abs_diff_lat + abs_diff_lon\n",
    "    min_diff_idx = combined_diff.argmin()\n",
    "    nearest_lat_idx, nearest_lon_idx = np.unravel_index(min_diff_idx, latitudes.shape)\n",
    "    return nearest_lat_idx, nearest_lon_idx\n",
    "\n",
    "def process_reanalysis_station(ds, station_id, station_name, orig_lat, orig_lon, variable, output_dir):\n",
    "    \"\"\"Extract raster values for a specific variable at a specified station and save to CSV.\"\"\"\n",
    "    if variable not in ds:\n",
    "        raise ValueError(f\"Variable {variable} not found in the dataset\")\n",
    "\n",
    "    # Find the nearest grid point using the lat-lon variables in the dataset\n",
    "    nearest_lat_idx, nearest_lon_idx = find_nearest_grid_point(ds, orig_lat, orig_lon)\n",
    "    \n",
    "    # Extract the data for the nearest grid point\n",
    "    point_data = ds[variable].isel(rlat=nearest_lat_idx, rlon=nearest_lon_idx).to_dataframe().reset_index()\n",
    "\n",
    "    # Add station ID, original latitude and longitude\n",
    "    point_data['station_ID'] = station_id\n",
    "    point_data['latitude'] = orig_lat\n",
    "    point_data['longitude'] = orig_lon\n",
    "\n",
    "    # Calculate daily averages\n",
    "    point_data['time'] = pd.to_datetime(point_data['time'])\n",
    "    numeric_cols = point_data.select_dtypes(include='number').columns\n",
    "    daily_data = point_data.resample('D', on='time')[numeric_cols].mean().reset_index()\n",
    "    daily_data['station_ID'] = station_id\n",
    "    daily_data['latitude'] = orig_lat\n",
    "    daily_data['longitude'] = orig_lon\n",
    "\n",
    "    # Save hourly results to CSV\n",
    "    hourly_output_path = os.path.join(output_dir, f\"station_{str(station_name)}_hourly.csv\")\n",
    "    if os.path.exists(hourly_output_path):\n",
    "        existing_df = pd.read_csv(hourly_output_path, parse_dates=['time'])\n",
    "        combined_df = pd.concat([existing_df, point_data], ignore_index=True)\n",
    "        combined_df.to_csv(hourly_output_path, index=False)\n",
    "    else:\n",
    "        point_data.to_csv(hourly_output_path, index=False)\n",
    "\n",
    "    # Save daily average results to CSV\n",
    "    daily_output_path = os.path.join(output_dir, f\"station_{str(station_name)}_daily.csv\")\n",
    "    if os.path.exists(daily_output_path):\n",
    "        existing_df = pd.read_csv(daily_output_path, parse_dates=['time'])\n",
    "        combined_df = pd.concat([existing_df, daily_data], ignore_index=True)\n",
    "        combined_df.to_csv(daily_output_path, index=False)\n",
    "    else:\n",
    "        daily_data.to_csv(daily_output_path, index=False)\n",
    "\n",
    "def extract_reanalysis_station_data(csv_path, nc_dir, output_dir, variable):\n",
    "    \"\"\"Extract data from all NetCDF files in a directory for each station and save results.\"\"\"\n",
    "    points_df = load_points(csv_path)\n",
    "\n",
    "    # Load and concatenate all NetCDF files\n",
    "    nc_files = [os.path.join(nc_dir, f) for f in os.listdir(nc_dir) if f.endswith('.nc')]\n",
    "    ds = xr.open_mfdataset(nc_files, combine='by_coords')\n",
    "\n",
    "    for index, row in points_df.iterrows():\n",
    "        station_id = row['station_ID']\n",
    "        station_name = str(row['Climate ID']),\n",
    "        station_name = station_name[0].replace(' ','')\n",
    "        orig_lat = row['latitude']\n",
    "        orig_lon = row['longitude']\n",
    "        print(f\"Processing station {station_name}\")\n",
    "        process_reanalysis_station(ds, station_id, station_name, orig_lat, orig_lon, variable, output_dir)\n",
    "\n",
    "# Execute extraction \n",
    "extract_reanalysis_station_data(undercatch_stations_update_csv, reanalysis_data_path, reanalysis_station_path, reanalysis_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge reanalysis data with SC-Earth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed station 3020610\n",
      "Processed station 3012050\n",
      "Processed station 3050519\n",
      "Processed station 3031480\n",
      "Processed station 3050778\n",
      "Processed station 3051R4R\n",
      "Processed station 3026KNQ\n",
      "Processed station 3024925\n",
      "Processed station 3025297\n",
      "Processed station 3011892\n",
      "Processed station 3053536\n",
      "Processed station 3034795\n",
      "Processed station 3030720\n",
      "Processed station 3036205\n",
      "Processed station 3023200\n",
      "Processed station 3023740\n",
      "Processed station 3035208\n",
      "Processed station 3031092\n",
      "Processed station 3031093\n",
      "Processed station 1176755\n",
      "Processed station 3017282\n",
      "Processed station 3015523\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_stations(csv_file_path):\n",
    "    \"\"\"Load station data from a CSV file.\"\"\"\n",
    "    stations_df = pd.read_csv(csv_file_path)\n",
    "    stations = stations_df[['Climate ID', 'latitude', 'longitude']].values\n",
    "    return stations\n",
    "\n",
    "def extract_reanalysis_at_stations_and_compute_means(ds, stations, output_dir):\n",
    "    \"\"\"Extract data from the dataset for all stations and compute means.\"\"\"\n",
    "    for station in stations:\n",
    "        climate_id, lat, lon = station\n",
    "        point = ds.sel(latitude=lat, longitude=lon, method='nearest').to_dataframe().reset_index()\n",
    "        point['time'] = pd.to_datetime(point['time'])\n",
    "        \n",
    "        hourly_mean = point\n",
    "        daily_mean = point.resample('D', on='time').mean().reset_index()\n",
    "        \n",
    "        hourly_mean.to_csv(f'{output_dir}/station_{climate_id}_hourly.csv', index=False)\n",
    "        daily_mean.to_csv(f'{output_dir}/station_{climate_id}_daily.csv', index=False)\n",
    "        print(f\"Processed station {climate_id}\")\n",
    " \n",
    "stations = load_stations(undercatch_stations_update_csv)\n",
    "nc_file_paths = glob.glob(f'{reanalysis_data_path}/*.nc')\n",
    "ds = xr.open_mfdataset(nc_file_paths, combine='by_coords')\n",
    "\n",
    "extract_reanalysis_at_stations_and_compute_means(ds, stations, reanalysis_station_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3020610_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3012050_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3023200_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3025297_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3030720_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3011892_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_1176755_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3051R4R_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3031093_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3050519_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3053536_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3017282_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3031480_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3034795_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3031092_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3024925_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3026KNQ_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3015523_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3036205_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3035208_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3023740_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3050778_sc_earth_with_wind.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_daily_wind_reanalysis_to_sc_earth_files(daily_dir, station_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Merges the \"windspd\" variable from daily CSV files into station CSV files based on matching station IDs.\n",
    "    \n",
    "    Parameters:\n",
    "    - daily_dir: str, path to the directory containing daily CSV files.\n",
    "    - station_dir: str, path to the directory containing station CSV files.\n",
    "    - output_dir: str, path to the directory to save the merged CSV files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get list of daily and station files\n",
    "    daily_files = {os.path.splitext(f)[0]: f for f in os.listdir(daily_dir) if f.endswith('daily.csv')}\n",
    "    # Remove the daily from all daily files\n",
    "    daily_files = {k.replace('_daily',''): v for k,v in daily_files.items()}\n",
    "\n",
    "    station_files = {os.path.splitext(f)[0]: f for f in os.listdir(station_dir) if f.endswith('.csv')}\n",
    "    \n",
    "    # Find matching station IDs\n",
    "    matching_ids = set(daily_files.keys()) & set(station_files.keys())\n",
    "    \n",
    "    for station_id in matching_ids:\n",
    "        daily_file_path = os.path.join(daily_dir, daily_files[station_id])\n",
    "        station_file_path = os.path.join(station_dir, station_files[station_id])\n",
    "        output_file_path = os.path.join(output_dir, f'{station_id}_sc_earth_with_wind.csv')\n",
    "        \n",
    "        # Load the CSV files\n",
    "        daily_df = pd.read_csv(daily_file_path)\n",
    "        station_df = pd.read_csv(station_file_path)\n",
    "\n",
    "        # Ensure the 'time' columns are in the same format\n",
    "        daily_df['time'] = pd.to_datetime(daily_df['time'])\n",
    "        station_df['time'] = pd.to_datetime(station_df['time'])\n",
    "\n",
    "        # Merge the dataframes on the 'time' column\n",
    "        merged_df = pd.merge(station_df, daily_df[['time', 'windspd']], on='time', how='left')\n",
    "\n",
    "        # Rename the column to 'reanalysis_wind'\n",
    "        merged_df.rename(columns={'windspd': 'reanalysis_wind'}, inplace=True)\n",
    "\n",
    "        # Save the merged DataFrame to a new CSV file\n",
    "        merged_df.to_csv(output_file_path, index=False)\n",
    "        print(f\"Saved merged data to {output_file_path}\")\n",
    "\n",
    "merge_daily_wind_reanalysis_to_sc_earth_files(reanalysis_station_path, sc_earth_path,interim_station_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/qfhdls_x5jj48jhgnhtfv1lm0000gn/T/ipykernel_3247/1725123043.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(hourly_data_file, delim_whitespace=True, skiprows=2, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3050519_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3050519_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3050519_UTF_hly_prec.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/qfhdls_x5jj48jhgnhtfv1lm0000gn/T/ipykernel_3247/1725123043.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(hourly_data_file, delim_whitespace=True, skiprows=2, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3050778_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3050778_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3050778_UTF_hly_prec.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/qfhdls_x5jj48jhgnhtfv1lm0000gn/T/ipykernel_3247/1725123043.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(hourly_data_file, delim_whitespace=True, skiprows=2, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3053536_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3053536_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3053536_UTF_hly_prec.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/qfhdls_x5jj48jhgnhtfv1lm0000gn/T/ipykernel_3247/1725123043.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(hourly_data_file, delim_whitespace=True, skiprows=2, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3035208_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3035208_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3035208_UTF_hly_prec.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/qfhdls_x5jj48jhgnhtfv1lm0000gn/T/ipykernel_3247/1725123043.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(hourly_data_file, delim_whitespace=True, skiprows=2, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/1176755_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/1176755_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/1176755_UTF_hly_prec.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/qfhdls_x5jj48jhgnhtfv1lm0000gn/T/ipykernel_3247/1725123043.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(hourly_data_file, delim_whitespace=True, skiprows=2, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3015523_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3015523_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3015523_UTF_hly_prec.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/BANFFCS.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/BOWVALLEY.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/JASPERWARDEN.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/PINCHERCREEKCLIMATE.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/REVELSTOKEAIRPORTAUTO.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/ROCKYMTNHOUSE(AUT).csv\n"
     ]
    }
   ],
   "source": [
    "def convert_txt_to_csv(txt_file, csv_file):\n",
    "    # Read the .txt file\n",
    "    with open(txt_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Assume the first non-empty line is the header and skip the second header line\n",
    "    header = next(line for line in lines if line.strip())\n",
    "    lines = lines[lines.index(header) + 2:]  # Skip the header line and the next line\n",
    "\n",
    "    # Write the content to a .csv file with the extracted header\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header.split())\n",
    "\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                writer.writerow(line.split())\n",
    "\n",
    "def calculate_daily_averages_from_eccc_paper(hourly_data_file, daily_data_file):\n",
    "\n",
    "    # Define columns\n",
    "    columns = [\"YYYYMMDDThhmm\", \"Unadj_P(mm)\", \"Tair(C)\", \"Wind(m/s)\", \"Wind_Flag\", \n",
    "               \"CE\", \"UTF_Adj_P(mm)\", \"CODECON(mm)\", \"UTF_Adj+CODECON_P(mm)\", \"Adj_Flag\"]\n",
    "    \n",
    "    # Read the data file\n",
    "    df = pd.read_csv(hourly_data_file, delim_whitespace=True, skiprows=2, names=columns)\n",
    "    \n",
    "    # Replace -99999 with NaN\n",
    "    df.replace(-99999, np.nan, inplace=True)\n",
    "\n",
    "    # Read date column to datetime\n",
    "    df['YYYYMMDDThhmm'] = pd.to_datetime(df['YYYYMMDDThhmm'], format='%Y%m%dT%H%M')\n",
    "\n",
    "    # Shift the date column back by 1 hour\n",
    "    df['YYYYMMDDThhmm'] = df['YYYYMMDDThhmm'] + pd.Timedelta(hours=1)\n",
    "\n",
    "    # Write YYYYMMDD to Date column\n",
    "    df['Date'] = df['YYYYMMDDThhmm'].dt.strftime('%Y%m%d')\n",
    "    \n",
    "    # Extract date part from datetime\n",
    "    #df['Date'] = df['YYYYMMDDThhmm'].str[:8]\n",
    "    \n",
    "    # Calculate daily averages, ignoring flags\n",
    "    daily_avg = df.groupby('Date').agg({\n",
    "        \"Unadj_P(mm)\": \"sum\",\n",
    "        \"Tair(C)\": \"mean\",\n",
    "        \"Wind(m/s)\": \"mean\",\n",
    "        \"CE\": \"mean\",\n",
    "        \"UTF_Adj_P(mm)\": \"sum\",\n",
    "        \"CODECON(mm)\": \"mean\",\n",
    "        \"UTF_Adj+CODECON_P(mm)\": \"sum\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Write daily averages to a CSV file\n",
    "    daily_avg.to_csv(daily_data_file, index=False)\n",
    "    print(f\"Daily averages have been written to {daily_data_file}\")\n",
    "\n",
    "\n",
    "def read_station_id_list(csv_file, column_name):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Return a list of the entries in the specified column\n",
    "    return df[column_name].tolist()\n",
    "\n",
    "def select_and_convert_raw_data_stations(station_id_list,hourly_path, daily_path):\n",
    "    for station in station_id_list:\n",
    "        station = str(station)\n",
    "        for file_name in os.listdir(hourly_path):\n",
    "            if station in file_name and file_name.endswith('.txt'):\n",
    "                # Construct the full file paths\n",
    "                hourly_file = os.path.join(hourly_path, file_name)\n",
    "                daily_file = os.path.join(daily_path, file_name)\n",
    "\n",
    "                #Update .txt to .csv\n",
    "                daily_file = daily_file.replace('.txt', '.csv')\n",
    "\n",
    "                # Define the destination .csv file name\n",
    "                calculate_daily_averages_from_eccc_paper(hourly_file, daily_file)\n",
    "                print(f\"Converted and copied: {hourly_file} to {daily_file}\")\n",
    "\n",
    "\n",
    "def merge_obs_and_model_csv_files(station_id_list, smith_2019_daily_path, gsod_path, merged_station_path,station_name_list):\n",
    "\n",
    "    for station, name in zip(station_id_list,station_name_list):\n",
    "        station = str(station)\n",
    "        for file_name in os.listdir(smith_2019_daily_path):\n",
    "            if station in file_name and file_name.endswith('.csv'):\n",
    "                # Construct the full file paths\n",
    "                obs_file = os.path.join(smith_2019_daily_path, file_name)\n",
    "                model_file = os.path.join(gsod_path, f'station_{station}_sc_earth_with_wind.csv')\n",
    "                df1 = pd.read_csv(obs_file)\n",
    "                df2 = pd.read_csv(model_file)\n",
    "\n",
    "                # Rename the date columns to a common name\n",
    "                df1 = df1.rename(columns={'Date': 'date'})\n",
    "                df2 = df2.rename(columns={'time': 'date'})\n",
    "\n",
    "                # Strip any leading/trailing whitespace from the date column in df1\n",
    "                df1['date'] = df1['date'].astype(str).str.strip()\n",
    "                \n",
    "                # Convert the 'YYYYMMDD' date format in df1 to a proper datetime format\n",
    "                df1['date'] = pd.to_datetime(df1['date'])\n",
    "                \n",
    "                # Convert the date column in df2 to datetime format\n",
    "                df2['date'] = pd.to_datetime(df2['date'], format='%Y-%m-%d')\n",
    "                \n",
    "                # Convert all other columns to numeric, coercing errors to NaN\n",
    "                for col in df1.columns:\n",
    "                    if col != 'date':\n",
    "                        df1[col] = pd.to_numeric(df1[col], errors='coerce')\n",
    "                \n",
    "                for col in df2.columns:\n",
    "                    if col != 'date':\n",
    "                        df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "                \n",
    "                # Merge the dataframes on the 'date' column\n",
    "                merged_df = pd.merge(df1, df2, on='date', how='outer')\n",
    "                \n",
    "                # Drop rows where there are NaN values in any of the columns\n",
    "                merged_df = merged_df.dropna()\n",
    "                \n",
    "                # Save the merged DataFrame to a new CSV file\n",
    "                merged_df.to_csv(Path(merged_station_path, f'{name}.csv'), index=False)\n",
    "                print(f\"Merged data saved to {merged_station_path}/{name}.csv\")\n",
    "\n",
    "station_id_list = read_station_id_list(evaluation_stations_csv, 'Climate ID')\n",
    "station_name_list = read_station_id_list(evaluation_stations_csv, 'Name')\n",
    "station_name_list = [s.replace(' ', '') for s in station_name_list]\n",
    "\n",
    "\n",
    "select_and_convert_raw_data_stations(station_id_list, smith_2019_hourly_path, smith_2019_daily_path)\n",
    "merge_obs_and_model_csv_files(station_id_list, smith_2019_daily_path, interim_station_path, merged_station_path, station_name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_station_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndercatch calculated for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Process the CSV files in the directory\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m calculate_undercatch_for_gsod_stations_with_reanalysis(\u001b[43mmerged_station_path\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_station_path' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_undercatch_for_gsod_stations(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Ensure required columns are present\n",
    "            if all(col in df.columns for col in ['wind', 'tmean', 'prcp']):\n",
    "                # Calculate CE and corrected precipitation\n",
    "                df['CE'] = df.apply(lambda row: calculate_CE(row['wind'], row['tmean']), axis=1)\n",
    "                df['corrected_prcp'] = df.apply(lambda row: apply_undercatch(row['prcp'], row['CE']), axis=1)\n",
    "                #Update so that if corrected_prcp is 0, set CE to 1\n",
    "                #df.loc[df['corrected_prcp'] == 0, 'CE'] = 1\n",
    "                \n",
    "                # Save the updated DataFrame back to the CSV file\n",
    "                df.to_csv(file_path, index=False)\n",
    "                print(f\"Undercatch calculated for {file_path}\")\n",
    "\n",
    "# Process the CSV files in the directory\n",
    "#calculate_undercatch_for_gsod_stations(output_station_path)\n",
    "\n",
    "def calculate_undercatch_for_gsod_stations_with_reanalysis(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Ensure required columns are present\n",
    "            if all(col in df.columns for col in ['wind', 'tmean', 'prcp','reanalysis_wind']):\n",
    "                # Calculate CE and corrected precipitation\n",
    "                df['CE_sc_earth'] = df.apply(lambda row: calculate_CE(row['wind'], row['tmean']), axis=1)\n",
    "                df['corrected_prcp_sc_earth'] = df.apply(lambda row: apply_undercatch(row['prcp'], row['CE_sc_earth']), axis=1)\n",
    "\n",
    "                df['CE_sc_reanalysis'] = df.apply(lambda row: calculate_CE(row['reanalysis_wind'], row['tmean']), axis=1)\n",
    "                df['corrected_prcp_sc_reanalysis'] = df.apply(lambda row: apply_undercatch(row['prcp'], row['CE_sc_reanalysis']), axis=1)\n",
    "\n",
    "                df['CE_eccc_reanalysis'] = df.apply(lambda row: calculate_CE(row['reanalysis_wind'], row['Tair(C)']), axis=1)\n",
    "                df['corrected_prcp_eccc_reanalysis'] = df.apply(lambda row: apply_undercatch(row['Unadj_P(mm)'], row['CE_eccc_reanalysis']), axis=1)\n",
    "\n",
    "\n",
    "                # Save the updated DataFrame back to the CSV file\n",
    "                df.to_csv(file_path, index=False)\n",
    "                print(f\"Undercatch calculated for {file_path}\")\n",
    "\n",
    "# Process the CSV files in the directory\n",
    "calculate_undercatch_for_gsod_stations_with_reanalysis(merged_station_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_accumulated_precipitation.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def generate_accumulated_precipitation_plots_per_water_year(input_csv_path,station,output_dir):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Convert date column to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Add a water year column\n",
    "    df['water_year'] = df['date'].apply(lambda x: x.year if x.month < 10 else x.year + 1)\n",
    "    \n",
    "    # Get unique water years\n",
    "    water_years = df['water_year'].unique()\n",
    "    \n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    num_plots = len(water_years)\n",
    "    num_cols = math.ceil(math.sqrt(num_plots))\n",
    "    num_rows = math.ceil(num_plots / num_cols)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(24, 6 * num_rows), sharex=False)\n",
    "\n",
    "    # Flatten axes array for easy iteration\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # Calculate accumulated precipitation for each water year and plot\n",
    "    for ax, water_year in zip(axes, water_years):\n",
    "        group = df[df['water_year'] == water_year].sort_values(by='date')\n",
    "        group['Accum_Unadj_P(mm)'] = group['Unadj_P(mm)'].cumsum()\n",
    "        group['Accum_prcp'] = group['prcp'].cumsum()\n",
    "        group['Accum_UTF_Adj_P(mm)'] = group['UTF_Adj_P(mm)'].cumsum()\n",
    "        group['Accum_corrected_prcp_sc_earth'] = group['corrected_prcp_sc_earth'].cumsum()\n",
    "        group['Accum_corrected_prcp_sc_reanalysis'] = group['corrected_prcp_sc_reanalysis'].cumsum()\n",
    "        group['Accum_corrected_prcp_eccc_reanalysis'] = group['corrected_prcp_eccc_reanalysis'].cumsum()\n",
    "\n",
    "        ax.plot(group['date'], group['Accum_Unadj_P(mm)'], label='Raw Gauge (ECCC)', linestyle='-', color='red')\n",
    "        ax.plot(group['date'], group['Accum_prcp'], label='Raw Gauge (SC-Earth)', linestyle='-', color='blue')\n",
    "        ax.plot(group['date'], group['Accum_UTF_Adj_P(mm)'], label='Undercatch Corrected (ECCC)', linestyle=':', color='red')\n",
    "        ax.plot(group['date'], group['Accum_corrected_prcp_sc_earth'], label='Undercatch Corrected (SC-Earth)', linestyle=':', color='blue')\n",
    "        ax.plot(group['date'], group['Accum_corrected_prcp_sc_reanalysis'], label='Undercatch Corrected (SC-Earth+ERA5 Wind)', linestyle=':', color='purple')\n",
    "        ax.plot(group['date'], group['Accum_corrected_prcp_eccc_reanalysis'], label='Undercatch Corrected (ECCC+ERA5 Wind)', linestyle=':', color='green')\n",
    "        ax.set_xlabel('Month')\n",
    "        ax.set_ylabel('Accumulated Precipitation (mm)')\n",
    "        ax.set_title(f'Accumulated Precipitation (Water Year {water_year})')\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Format x-axis to display only the month\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "        # Maintain the right x-axis bound\n",
    "        ax.set_xlim([group['date'].min(), group['date'].max()])\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_plots, num_rows * num_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    # Add a single legend at the bottom of the figure\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=3)\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 1])  # Adjust rect to make space for the legend\n",
    "\n",
    "    # Save the combined plot to a PNG file\n",
    "    output_plot_path = os.path.join(output_dir, f'{station}_accumulated_precipitation.png')\n",
    "    print(f\"Saving plot to {output_plot_path}\")\n",
    "    plt.savefig(output_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "# Run the function to generate the plots\n",
    "\n",
    "for station in station_name_list:\n",
    "    station = str(station)\n",
    "    comparative_file = Path(merged_station_path,f'{station}.csv')\n",
    "\n",
    "    generate_accumulated_precipitation_plots_per_water_year(comparative_file,station,plot_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_CE.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_ce_boxplot(file_path,station,output_dir,legend_title='Method', method_labels=None):\n",
    "    \"\"\"\n",
    "    Plots a boxplot of CE for each method aggregated by month.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the CSV file\n",
    "    - legend_title: str, title for the legend\n",
    "    - method_labels: dict, dictionary to map method column names to desired labels\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert the date column to datetime format\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "    # Extract the month from the date column\n",
    "    data['month'] = data['date'].dt.month\n",
    "\n",
    "    # Melt the dataset to have one column for method and another for CE values\n",
    "    melted_data = pd.melt(data, id_vars=['month'], value_vars=['CE', 'CE_sc_earth','CE_sc_reanalysis','CE_eccc_reanalysis'], var_name='method', value_name='CE_calc')\n",
    "    #Drop all values where CE is 1\n",
    "    melted_data = melted_data[melted_data['CE_calc'] != 1]\n",
    "\n",
    "    # Apply method labels if provided\n",
    "    if method_labels:\n",
    "        melted_data['method'] = melted_data['method'].map(method_labels)\n",
    "\n",
    "    # Create the boxplot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='month', y='CE_calc', hue='method', data=melted_data)\n",
    "    plt.title(f'Boxplot of CE for {station}')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('CE')\n",
    "    plt.legend(title=legend_title)\n",
    "    plt.grid(True)\n",
    "    # Save the plot to a PNG file\n",
    "    output_plot_path = f'{output_dir}/{station}_CE.png'\n",
    "    print(f'Saving plot to {output_plot_path}')\n",
    "    plt.savefig(output_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "# Usage example\n",
    "for station in station_name_list:\n",
    "    station = str(station)\n",
    "    comparative_file = Path(merged_station_path,f'{station}.csv')\n",
    "    \n",
    "    method_labels = {'CE': 'ECCC Analysis', 'CE_sc_earth': 'SC-Earth','CE_eccc_reanalysis': 'ECCC-ERA5','CE_sc_reanalysis': 'SC-Earth-ERA5'}\n",
    "    plot_ce_boxplot(comparative_file,station,plot_path,legend_title='CE Methods', method_labels=method_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_reanalysis_wind.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def generate_comparative_plots(file_path, col1, col2, output_dir, station_name, x_label=None, y_label=None):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Plot correlation with regression line\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(x=col1, y=col2, data=df, line_kws={\"color\": \"red\", \"alpha\": 0.7, \"lw\": 2})\n",
    "    \n",
    "    # Calculate correlation coefficient and regression equation\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(df[col1], df[col2])\n",
    "    regression_formula = f'Y = {intercept:.2f} + {slope:.2f}X'\n",
    "    correlation_coefficient = f'R^2 = {r_value**2:.2f}'\n",
    "    \n",
    "        # Add a 1:1 line for reference\n",
    "    max_val = max(df[col1].max(), df[col2].max())\n",
    "    min_val = min(df[col1].min(), df[col2].min())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='blue', linestyle='--', linewidth=1, label='1:1 Line')\n",
    "    \n",
    "    # Annotate plot with regression equation and R^2\n",
    "    plt.text(0.05, 0.95, regression_formula, transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "    plt.text(0.05, 0.90, correlation_coefficient, transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "    \n",
    "    # Set labels\n",
    "    plt.title(f'Correlation between {col1} and {col2}')\n",
    "    plt.xlabel(x_label if x_label else col1)\n",
    "    plt.ylabel(y_label if y_label else col2)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Define the output file path\n",
    "    output_file = os.path.join(output_dir, f'{station_name}_{col2}.png')\n",
    "    \n",
    "    # Save the plot to the specified directory\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {output_file}\")\n",
    "\n",
    "for station in station_name_list:\n",
    "    station = str(station)\n",
    "    comparative_file = Path(merged_station_path,f'{station}.csv')\n",
    "    generate_comparative_plots(comparative_file, 'Wind(m/s)', 'wind',plot_path, station, x_label='ECCC Wind', y_label='EC-Earth Wind')\n",
    "    generate_comparative_plots(comparative_file, 'Tair(C)', 'tmean', plot_path, station, x_label='ECCC Temperature (°C)', y_label='EC-Earth Temperature (°C)')\n",
    "    generate_comparative_plots(comparative_file, 'Wind(m/s)', 'reanalysis_wind',plot_path, station, x_label='ECCC Wind', y_label='reanalysis Wind')\n",
    "    #generate_comparative_plots(comparative_file, 'Tair(C)', 'tmean', plot_path, station, x_label='ECCC Temperature (°C)', y_label='EC-Earth Temperature (°C)')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_wind_distribution.png\n"
     ]
    }
   ],
   "source": [
    "def plot_histogram(df, columns,labels,plot_path,station_name, bins=30):\n",
    "    \"\"\"\n",
    "    Plots overlapping histograms for the specified columns in the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    columns (list): List of columns to plot.\n",
    "    bins (int): Number of bins for the histogram.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for column,label in zip(columns, labels):\n",
    "        plt.hist(df[column], bins=bins, alpha=0.5, label=label)\n",
    "    \n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Station {station_name} Wind Distribution')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    #plt.show()\n",
    "    # Define the output file path\n",
    "    output_file = os.path.join(plot_path, f'{station_name}_wind_distribution.png')\n",
    "    \n",
    "    # Save the plot to the specified directory\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {output_file}\")\n",
    "\n",
    "# Plot overlapping histograms for 'Wind(m/s)', 'wind', and 'reanalysis_wind'\n",
    "columns_to_plot = ['Wind(m/s)', 'wind', 'reanalysis_wind']\n",
    "labels = ['ECCC Wind', 'EC-Earth Wind', 'Reanalysis Wind']\n",
    "for station in station_name_list:\n",
    "    station = str(station)\n",
    "    comparative_file = Path(merged_station_path,f'{station}.csv')\n",
    "    df = pd.read_csv(comparative_file)\n",
    "    plot_histogram(df, columns_to_plot,labels, plot_path, station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_wind_distribution.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_histogram(df, columns, labels, plot_path, station_name, bins):\n",
    "    \"\"\"\n",
    "    Plots overlapping histograms for the specified columns in the dataframe using the provided bins.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    columns (list): List of columns to plot.\n",
    "    bins (array): Array of bin edges for the histogram.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for column, label in zip(columns, labels):\n",
    "        plt.hist(df[column], bins=bins, alpha=0.5, label=label)\n",
    "    \n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Station {station_name} Wind Distribution')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Define the output file path\n",
    "    output_file = os.path.join(plot_path, f'{station_name}_wind_distribution.png')\n",
    "    \n",
    "    # Save the plot to the specified directory\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {output_file}\")\n",
    "\n",
    "def determine_common_bins(df, columns, bins=30):\n",
    "    \"\"\"\n",
    "    Determines common bin edges for all columns in the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    columns (list): List of columns to consider for bin edges.\n",
    "    bins (int): Number of bins for the histogram.\n",
    "    \n",
    "    Returns:\n",
    "    array: Array of bin edges.\n",
    "    \"\"\"\n",
    "    min_value = df[columns].min().min()\n",
    "    max_value = df[columns].max().max()\n",
    "    bin_edges = np.linspace(min_value, max_value, bins + 1)\n",
    "    return bin_edges\n",
    "\n",
    "station_name_list = read_station_id_list(evaluation_stations_csv, 'Name')\n",
    "station_name_list = [s.replace(' ', '') for s in station_name_list]\n",
    "columns_to_plot = ['Wind(m/s)', 'wind', 'reanalysis_wind']\n",
    "labels = ['ECCC Wind', 'EC-Earth Wind', 'Reanalysis Wind']\n",
    "\n",
    "# Plot histograms for each station\n",
    "for station in station_name_list:\n",
    "    station = str(station)\n",
    "    comparative_file = Path(merged_station_path, f'{station}.csv')\n",
    "    df = pd.read_csv(comparative_file)\n",
    "    common_bins = determine_common_bins(df, columns_to_plot)\n",
    "    plot_histogram(df, columns_to_plot, labels, plot_path, station, common_bins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpep_snakemake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
