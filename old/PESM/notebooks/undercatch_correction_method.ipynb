{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undercatch Correction - Verification of Method ##\n",
    "\n",
    "Implement undercatch corrections to station data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Needed Packages ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from undercatch_processing import *\n",
    "\n",
    "# add autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input file locations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Paths\n",
    "input_path = '/Users/dcasson/Data/pems/'\n",
    "output_path = '/Users/dcasson/Data/pems/undercatch/'\n",
    "input_station_data_path = Path(input_path,'station_data')\n",
    "\n",
    "# Define variables\n",
    "first_filter_year = 2004\n",
    "reanalysis_variable = 'windspd'\n",
    "\n",
    "#Input station data\n",
    "cdn_complete_stations = Path(input_station_data_path,'Station Inventory EN.csv')\n",
    "gsod_stations = Path(input_station_data_path,'isd-history.csv')\n",
    "ghcnd_stations = Path(input_station_data_path,'ghcnd-stations.csv')\n",
    "\n",
    "# Input SC-Earth data\n",
    "undercatch_stations_nc = Path(input_path,'sc_earth','undercatch_stations.nc')\n",
    "reanalysis_data_path = Path(input_path,'era5')\n",
    "\n",
    "# Smith, 2019 data\n",
    "smith_2019_stations = Path(input_station_data_path,'EN_StationCatalogue_2019.csv')\n",
    "smith_2019_hourly_path = Path(input_path,'smith_2019/hourly_data/')\n",
    "smith_2019_daily_path = Path(input_path,'smith_2019/daily_data/')\n",
    "\n",
    "# Define paths\n",
    "sc_earth_path = Path(output_path,'sc_earth')\n",
    "interim_station_path = Path(output_path,'interim_stations')\n",
    "reanalysis_station_path = Path(output_path,'reanalysis_stations')\n",
    "merged_station_path = Path(output_path,'merged_stations')\n",
    "undercatch_station_path = Path(output_path,'undercatch_stations_test')\n",
    "plot_path = Path(output_path,'plots')\n",
    "\n",
    "\n",
    "#Output files\n",
    "cdn_year_filtered_stations = Path(output_path,'cdn_station_inventory.csv')\n",
    "undercatch_stations_csv = Path(output_path,'undercatch_from_nc.csv')\n",
    "undercatch_stations_update_csv = Path(output_path,'undercatch_stations_update.csv')\n",
    "evaluation_stations_csv = Path(output_path,'evaluation_stations.csv')\n",
    "\n",
    "# Make output directories\n",
    "os.makedirs(smith_2019_daily_path, exist_ok=True)\n",
    "os.makedirs(interim_station_path, exist_ok=True)\n",
    "os.makedirs(reanalysis_station_path, exist_ok=True)\n",
    "os.makedirs(merged_station_path, exist_ok=True)\n",
    "os.makedirs(sc_earth_path, exist_ok=True)\n",
    "os.makedirs(undercatch_station_path, exist_ok=True)\n",
    "os.makedirs(plot_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Canadian Large Dataset, to remove old stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered stations saved to /Users/dcasson/Data/pems/undercatch/cdn_station_inventory.csv\n"
     ]
    }
   ],
   "source": [
    "def filter_stations_by_year(input_csv_path, output_csv_path, year):\n",
    "    # Load the CSV file with appropriate delimiter and skip initial lines\n",
    "    stations_df = pd.read_csv(input_csv_path, delimiter=',', skiprows=3)\n",
    "    \n",
    "    # Filter out rows where 'DLY Last Year' is before the specified year\n",
    "    filtered_df = stations_df[stations_df['DLY Last Year'] >= year]\n",
    "    \n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    filtered_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Filtered stations saved to {output_csv_path}\")\n",
    "\n",
    "filter_stations_by_year(cdn_complete_stations, cdn_year_filtered_stations, first_filter_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read stations from SC-Earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undercatch stations saved to /Users/dcasson/Data/pems/undercatch/undercatch_from_nc.csv\n"
     ]
    }
   ],
   "source": [
    "def read_undercatch_stations(nc_file_path, output_csv_path):\n",
    "    # Open the netCDF file using xarray\n",
    "    ds = xr.open_dataset(nc_file_path)\n",
    "    \n",
    "    # Extract variables\n",
    "    station_ids = ds['station_ID'].values\n",
    "    latitudes = ds['latitude'].values\n",
    "    longitudes = ds['longitude'].values\n",
    "    elevations = ds['elevation'].values\n",
    "    \n",
    "    # Process station_IDs to remove prefix\n",
    "    processed_station_ids = [sid.split('_')[1][:5] for sid in station_ids]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'full_station_ID':station_ids,\n",
    "        'station_ID': processed_station_ids,\n",
    "        'latitude': latitudes,\n",
    "        'longitude': longitudes,\n",
    "        'elevation': elevations\n",
    "    })\n",
    "    \n",
    "    # Output to CSV\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Undercatch stations saved to {output_csv_path}\")\n",
    "\n",
    "read_undercatch_stations(undercatch_stations_nc, undercatch_stations_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge station metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined /Users/dcasson/Data/pems/undercatch/undercatch_from_nc.csv and /Users/dcasson/Data/pems/undercatch/cdn_station_inventory.csv saved to /Users/dcasson/Data/pems/undercatch/undercatch_stations_update.csv\n",
      "Joined /Users/dcasson/Data/pems/undercatch/undercatch_stations_update.csv and /Users/dcasson/Data/pems/station_data/EN_StationCatalogue_2019.csv saved to /Users/dcasson/Data/pems/undercatch/evaluation_stations.csv\n"
     ]
    }
   ],
   "source": [
    "def join_datasets(file1_path, file2_path, join_column_file1, join_column_file2, output_path, join_type='inner'):\n",
    "    \n",
    "    # Load the datasets\n",
    "    df1 = pd.read_csv(file1_path)\n",
    "    df2 = pd.read_csv(file2_path)\n",
    "\n",
    "    # Ensure the columns for joining are strings and strip trailing .0 if present\n",
    "    df1[join_column_file1] = df1[join_column_file1].astype(str).str.rstrip('.0')\n",
    "    df2[join_column_file2] = df2[join_column_file2].astype(str).str.rstrip('.0')\n",
    "\n",
    "    # Perform the join\n",
    "    merged_df = pd.merge(df1, df2, left_on=join_column_file1, right_on=join_column_file2, how=join_type)\n",
    "\n",
    "    # Save the result to CSV\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"Joined {file1_path} and {file2_path} saved to {output_path}\")\n",
    "\n",
    "join_datasets(undercatch_stations_csv, cdn_year_filtered_stations, 'station_ID', 'WMO ID', undercatch_stations_update_csv)\n",
    "\n",
    "join_datasets(undercatch_stations_update_csv,smith_2019_stations,'Climate ID', 'StationID', evaluation_stations_csv)\n",
    "\n",
    "#tuolumne_prcp = '/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/prcp_subset.csv'\n",
    "#ghcnd = '/Users/dcasson/Data/pems/station_data/ghcnd-stations.csv'\n",
    "#gsod = '/Users/dcasson/Data/pems/station_data/isd-history.csv'\n",
    "\n",
    "#join_datasets(tuolumne_prcp,ghcnd,'station_ID','StationID','/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/prcp_subset_ghcnd.csv')\n",
    "#join_datasets(tuolumne_prcp,gsod,'station_ID','USAF','/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/prcp_subset_gsod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/tuolumne_stations_ghcnd.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m geodesic\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load GHCNd stations metadata\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m ghcnd_stations \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/tuolumne_stations_ghcnd.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust the path as needed\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load CDEC stations metadata\u001b[39;00m\n\u001b[1;32m      8\u001b[0m cdec_stations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/dcasson/Data/pems/station_data/CDEC.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Adjust the path as needed\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/gpep_snakemake/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/gpep_snakemake/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/gpep_snakemake/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/gpep_snakemake/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/gpep_snakemake/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/tuolumne_stations_ghcnd.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load GHCNd stations metadata\n",
    "ghcnd_stations = pd.read_csv('/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/tuolumne_stations_ghcnd.csv')  # Adjust the path as needed\n",
    "\n",
    "# Load CDEC stations metadata\n",
    "cdec_stations = pd.read_csv('/Users/dcasson/Data/pems/station_data/CDEC.csv')  # Adjust the path as needed\n",
    "\n",
    "# Function to calculate distance between two coordinates\n",
    "def calculate_distance(coord1, coord2):\n",
    "    return geodesic(coord1, coord2).km\n",
    "\n",
    "# Create an empty list to store matches\n",
    "matches = []\n",
    "\n",
    "# Iterate through each GHCNd station\n",
    "for idx, ghcnd_station in ghcnd_stations.iterrows():\n",
    "    ghcnd_coords = (ghcnd_station['latitude'], ghcnd_station['longitude'])\n",
    "    closest_station = None\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    # Iterate through each CDEC station\n",
    "    for _, cdec_station in cdec_stations.iterrows():\n",
    "        cdec_coords = (cdec_station['Latitude'], cdec_station['Longitude'])\n",
    "        distance = calculate_distance(ghcnd_coords, cdec_coords)\n",
    "\n",
    "        # Check if the current CDEC station is the closest match\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_station = cdec_station\n",
    "\n",
    "    # If the closest station is within a reasonable distance, consider it a match\n",
    "    if min_distance < 3.0:  # Adjust the distance threshold as needed\n",
    "        matches.append({\n",
    "            'ghcnd_id': ghcnd_station['station_ID'],\n",
    "            'ghcnd_name': ghcnd_station['StationName'],\n",
    "            'cdec_id': closest_station['ID'],\n",
    "            'cdec_name': closest_station['Station Name'],\n",
    "            'cdec_operator': closest_station['Operator'],\n",
    "            'distance_km': min_distance\n",
    "        })\n",
    "\n",
    "# Convert matches to a DataFrame for easy analysis\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "# Save the matches to a CSV file\n",
    "matches_df.to_csv('/Users/dcasson/Data/gpep/tuolumne/data_prep/stations/matched_stations.csv', index=False)\n",
    "\n",
    "print('Matching complete. Results saved to matched_stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3010162.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3030540.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3020610.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3010650.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3012050.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3035342.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_301S001.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3050519.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3056214.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3031480.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3042045.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3032642.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3013640.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3030768.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3050778.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3051R4R.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3044533.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3026KNQ.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3024668.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3024925.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3025243.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3016124.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3011887.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3037675.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3025297.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3011892.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3053536.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3034795.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3030720.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3032422.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3030772.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3036205.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3033498.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3032550.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3035422.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3035650.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3023200.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3023740.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3035208.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_1145M29.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3032927.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3036500.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3031092.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3031093.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_1176755.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3017282.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3015523.csv\n",
      "Station data saved to /Users/dcasson/Data/pems/undercatch/sc_earth/station_3020405.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_sc_earth_station_data(nc_file_path, csv_file_path, output_dir):\n",
    "    # Load the netCDF file\n",
    "    ds = xr.open_dataset(nc_file_path)\n",
    "    \n",
    "    # Load the CSV file\n",
    "    stations_df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a dictionary to map station_ID to station_number\n",
    "    station_id_map = {str(ds['station_ID'][i].values): i for i in range(len(ds['station_ID']))}\n",
    "    \n",
    "    # Iterate through the stations in the CSV file\n",
    "    for _, row in stations_df.iterrows():\n",
    "        climate_id = row['Climate ID']\n",
    "        if not pd.isna(climate_id):\n",
    "            station_id = row['full_station_ID']\n",
    "            station_number = station_id_map.get(station_id, None)\n",
    "            \n",
    "            if station_number is not None:\n",
    "                # Extract data for the station\n",
    "                time = ds['time'].values\n",
    "                prcp = ds['prcp'].isel(station_number=station_number).values\n",
    "                tmean = ds['tmean'].isel(station_number=station_number).values\n",
    "                wind = ds['wind'].isel(station_number=station_number).values\n",
    "                \n",
    "                # Create a DataFrame for the station data\n",
    "                station_data = pd.DataFrame({\n",
    "                    'time': time,\n",
    "                    'prcp': prcp,\n",
    "                    'tmean': tmean,\n",
    "                    'wind': wind\n",
    "                })\n",
    "                \n",
    "                # Define the output file path\n",
    "                output_file_path = os.path.join(output_dir, f'station_{str(climate_id)}.csv')\n",
    "                \n",
    "                # Save the DataFrame to a CSV file\n",
    "                station_data.to_csv(output_file_path, index=False)\n",
    "                print(f\"Station data saved to {output_file_path}\")\n",
    "\n",
    "extract_sc_earth_station_data(undercatch_stations_nc, undercatch_stations_update_csv, sc_earth_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract reanalysis data for undercatch stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing station 3010162\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3010162_daily.csv\n",
      "Processing station 3030540\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3030540_daily.csv\n",
      "Processing station 3020610\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3020610_daily.csv\n",
      "Processing station 3010650\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3010650_daily.csv\n",
      "Processing station 3012050\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3012050_daily.csv\n",
      "Processing station 3035342\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3035342_daily.csv\n",
      "Processing station 301S001\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_301S001_daily.csv\n",
      "Processing station 3050519\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3050519_daily.csv\n",
      "Processing station 3056214\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3056214_daily.csv\n",
      "Processing station 3031480\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3031480_daily.csv\n",
      "Processing station 3042045\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3042045_daily.csv\n",
      "Processing station 3032642\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3032642_daily.csv\n",
      "Processing station 3013640\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3013640_daily.csv\n",
      "Processing station 3030768\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3030768_daily.csv\n",
      "Processing station 3050778\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3050778_daily.csv\n",
      "Processing station 3051R4R\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3051R4R_daily.csv\n",
      "Processing station 3044533\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3044533_daily.csv\n",
      "Processing station 3026KNQ\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3026KNQ_daily.csv\n",
      "Processing station 3024668\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3024668_daily.csv\n",
      "Processing station 3024925\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3024925_daily.csv\n",
      "Processing station 3025243\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3025243_daily.csv\n",
      "Processing station 3016124\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3016124_daily.csv\n",
      "Processing station 3011887\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3011887_daily.csv\n",
      "Processing station 3037675\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3037675_daily.csv\n",
      "Processing station 3025297\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3025297_daily.csv\n",
      "Processing station 3011892\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3011892_daily.csv\n",
      "Processing station 3053536\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3053536_daily.csv\n",
      "Processing station 3034795\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3034795_daily.csv\n",
      "Processing station 3030720\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3030720_daily.csv\n",
      "Processing station 3032422\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3032422_daily.csv\n",
      "Processing station 3030772\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3030772_daily.csv\n",
      "Processing station 3036205\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3036205_daily.csv\n",
      "Processing station 3033498\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3033498_daily.csv\n",
      "Processing station 3032550\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3032550_daily.csv\n",
      "Processing station 3035422\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3035422_daily.csv\n",
      "Processing station 3035650\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3035650_daily.csv\n",
      "Processing station 3023200\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3023200_daily.csv\n",
      "Processing station 3023740\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3023740_daily.csv\n",
      "Processing station 3035208\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3035208_daily.csv\n",
      "Processing station 1145M29\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_1145M29_daily.csv\n",
      "Processing station 3032927\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3032927_daily.csv\n",
      "Processing station 3036500\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3036500_daily.csv\n",
      "Processing station 3031092\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3031092_daily.csv\n",
      "Processing station 3031093\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3031093_daily.csv\n",
      "Processing station 1176755\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_1176755_daily.csv\n",
      "Processing station 3017282\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3017282_daily.csv\n",
      "Processing station 3015523\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3015523_daily.csv\n",
      "Processing station 3020405\n",
      "Output path:  /Users/dcasson/Data/pems/undercatch/reanalysis_stations/station_3020405_daily.csv\n"
     ]
    }
   ],
   "source": [
    "def load_points(csv_path):\n",
    "    \"\"\"Load points from CSV file.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Convert station_ID to string without tuple notation\n",
    "    df['station_ID'] = df['station_ID'].apply(lambda x: str(x).strip(\"(),'\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_nearest_grid_point(ds, orig_lat, orig_lon):\n",
    "    \"\"\"\n",
    "    Find the nearest grid point in the dataset for the given latitude and longitude.\n",
    "    \n",
    "    Parameters:\n",
    "    - ds: Dataset containing 'latitude' and 'longitude' coordinates.\n",
    "    - orig_lat: Original latitude value.\n",
    "    - orig_lon: Original longitude value.\n",
    "    \n",
    "    Returns:\n",
    "    - nearest_lat_idx: Index of the nearest latitude.\n",
    "    - nearest_lon_idx: Index of the nearest longitude.\n",
    "    \"\"\"\n",
    "    latitudes = ds['latitude'].values\n",
    "    longitudes = ds['longitude'].values\n",
    "    \n",
    "    # Find the index of the nearest latitude\n",
    "    nearest_lat_idx = np.abs(latitudes - orig_lat).argmin()\n",
    "    \n",
    "    # Find the index of the nearest longitude\n",
    "    nearest_lon_idx = np.abs(longitudes - orig_lon).argmin()\n",
    "    \n",
    "    return nearest_lat_idx, nearest_lon_idx\n",
    "\n",
    "def process_reanalysis_station(ds, station_id, station_name, orig_lat, orig_lon, variable, output_dir):\n",
    "    \"\"\"Extract raster values for a specific variable at a specified station and save to CSV.\"\"\"\n",
    "    if variable not in ds:\n",
    "        raise ValueError(f\"Variable {variable} not found in the dataset\")\n",
    "\n",
    "    # Find the nearest grid point using the lat-lon variables in the dataset\n",
    "    nearest_lat_idx, nearest_lon_idx = find_nearest_grid_point(ds, orig_lat, orig_lon)\n",
    "    \n",
    "    # Extract the data for the nearest grid point\n",
    "    point_data = ds[variable].isel(latitude=nearest_lat_idx, longitude=nearest_lon_idx).to_dataframe().reset_index()\n",
    "\n",
    "    # Add station ID, original latitude and longitude\n",
    "    point_data['station_ID'] = station_id\n",
    "    point_data['latitude'] = orig_lat\n",
    "    point_data['longitude'] = orig_lon\n",
    "\n",
    "    # Calculate daily averages\n",
    "    point_data['time'] = pd.to_datetime(point_data['time'])\n",
    "    numeric_cols = point_data.select_dtypes(include='number').columns\n",
    "    daily_data = point_data.resample('D', on='time')[numeric_cols].mean().reset_index()\n",
    "    daily_data['station_ID'] = station_id\n",
    "    daily_data['latitude'] = orig_lat\n",
    "    daily_data['longitude'] = orig_lon\n",
    "\n",
    "    # Save hourly results to CSV\n",
    "    hourly_output_path = os.path.join(output_dir, f\"station_{str(station_name)}_hourly.csv\")\n",
    "    if os.path.exists(hourly_output_path):\n",
    "        existing_df = pd.read_csv(hourly_output_path, parse_dates=['time'])\n",
    "        combined_df = pd.concat([existing_df, point_data], ignore_index=True)\n",
    "        combined_df.to_csv(hourly_output_path, index=False)\n",
    "    else:\n",
    "        point_data.to_csv(hourly_output_path, index=False)\n",
    "\n",
    "    # Save daily average results to CSV\n",
    "    daily_output_path = os.path.join(output_dir, f\"station_{str(station_name)}_daily.csv\")\n",
    "    print(\"Output path: \", daily_output_path)\n",
    "    if os.path.exists(daily_output_path):\n",
    "        existing_df = pd.read_csv(daily_output_path, parse_dates=['time'])\n",
    "        combined_df = pd.concat([existing_df, daily_data], ignore_index=True)\n",
    "        combined_df.to_csv(daily_output_path, index=False)\n",
    "    else:\n",
    "        daily_data.to_csv(daily_output_path, index=False)\n",
    "\n",
    "def extract_reanalysis_station_data(csv_path, nc_dir, output_dir, variable):\n",
    "    \"\"\"Extract data from all NetCDF files in a directory for each station and save results.\"\"\"\n",
    "    points_df = load_points(csv_path)\n",
    "\n",
    "    # Load and concatenate all NetCDF files\n",
    "    nc_files = [os.path.join(nc_dir, f) for f in os.listdir(nc_dir) if f.endswith('.nc')]\n",
    "    ds = xr.open_mfdataset(nc_files, combine='by_coords')\n",
    "\n",
    "    for index, row in points_df.iterrows():\n",
    "        station_id = row['station_ID']\n",
    "        station_name = str(row['Climate ID']),\n",
    "        station_name = station_name[0].replace(' ','')\n",
    "        orig_lat = row['latitude']\n",
    "        orig_lon = row['longitude']\n",
    "        print(f\"Processing station {station_name}\")\n",
    "        process_reanalysis_station(ds, station_id, station_name, orig_lat, orig_lon, variable, output_dir)\n",
    "\n",
    "# Execute extraction \n",
    "extract_reanalysis_station_data(undercatch_stations_update_csv, reanalysis_data_path, reanalysis_station_path, reanalysis_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge reanalysis data with SC-Earth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed station 3020610\n",
      "Processed station 3012050\n",
      "Processed station 3050519\n",
      "Processed station 3031480\n",
      "Processed station 3050778\n",
      "Processed station 3051R4R\n",
      "Processed station 3026KNQ\n",
      "Processed station 3024925\n",
      "Processed station 3025297\n",
      "Processed station 3011892\n",
      "Processed station 3053536\n",
      "Processed station 3034795\n",
      "Processed station 3030720\n",
      "Processed station 3036205\n",
      "Processed station 3023200\n",
      "Processed station 3023740\n",
      "Processed station 3035208\n",
      "Processed station 3031092\n",
      "Processed station 3031093\n",
      "Processed station 1176755\n",
      "Processed station 3017282\n",
      "Processed station 3015523\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_stations(csv_file_path):\n",
    "    \"\"\"Load station data from a CSV file.\"\"\"\n",
    "    stations_df = pd.read_csv(csv_file_path)\n",
    "    stations = stations_df[['Climate ID', 'latitude', 'longitude']].values\n",
    "    return stations\n",
    "\n",
    "def extract_reanalysis_at_stations_and_compute_means(ds, stations, output_dir):\n",
    "    \"\"\"Extract data from the dataset for all stations and compute means.\"\"\"\n",
    "    for station in stations:\n",
    "        climate_id, lat, lon = station\n",
    "        point = ds.sel(latitude=lat, longitude=lon, method='nearest').to_dataframe().reset_index()\n",
    "        point['time'] = pd.to_datetime(point['time'])\n",
    "        \n",
    "        hourly_mean = point\n",
    "        daily_mean = point.resample('D', on='time').mean().reset_index()\n",
    "        \n",
    "        hourly_mean.to_csv(f'{output_dir}/station_{climate_id}_hourly.csv', index=False)\n",
    "        daily_mean.to_csv(f'{output_dir}/station_{climate_id}_daily.csv', index=False)\n",
    "        print(f\"Processed station {climate_id}\")\n",
    " \n",
    "stations = load_stations(undercatch_stations_update_csv)\n",
    "nc_file_paths = glob.glob(f'{reanalysis_data_path}/*.nc')\n",
    "ds = xr.open_mfdataset(nc_file_paths, combine='by_coords')\n",
    "\n",
    "extract_reanalysis_at_stations_and_compute_means(ds, stations, reanalysis_station_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3020610_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3033498_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3012050_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3023200_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3030768_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3042045_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3035342_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3030540_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_1176755_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3031092_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3026KNQ_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3010650_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3025243_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3034795_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_1145M29_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3025297_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3011892_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3024668_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3015523_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3044533_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3031093_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3030772_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3010162_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3032927_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3053536_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3031480_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3023740_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3051R4R_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3050778_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3037675_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3030720_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3032422_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3035208_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3032642_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3050519_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3056214_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3036500_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3013640_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3011887_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3020405_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3035422_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3016124_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3036205_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_301S001_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3017282_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3024925_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3035650_sc_earth_with_wind.csv\n",
      "Saved merged data to /Users/dcasson/Data/pems/undercatch/interim_stations/station_3032550_sc_earth_with_wind.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_daily_wind_reanalysis_to_sc_earth_files(daily_dir, station_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Merges the \"windspd\" variable from daily CSV files into station CSV files based on matching station IDs.\n",
    "    \n",
    "    Parameters:\n",
    "    - daily_dir: str, path to the directory containing daily CSV files.\n",
    "    - station_dir: str, path to the directory containing station CSV files.\n",
    "    - output_dir: str, path to the directory to save the merged CSV files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get list of daily and station files\n",
    "    daily_files = {os.path.splitext(f)[0]: f for f in os.listdir(daily_dir) if f.endswith('daily.csv')}\n",
    "    # Remove the daily from all daily files\n",
    "    daily_files = {k.replace('_daily',''): v for k,v in daily_files.items()}\n",
    "\n",
    "    station_files = {os.path.splitext(f)[0]: f for f in os.listdir(station_dir) if f.endswith('.csv')}\n",
    "    \n",
    "    # Find matching station IDs\n",
    "    matching_ids = set(daily_files.keys()) & set(station_files.keys())\n",
    "    \n",
    "    for station_id in matching_ids:\n",
    "        daily_file_path = os.path.join(daily_dir, daily_files[station_id])\n",
    "        station_file_path = os.path.join(station_dir, station_files[station_id])\n",
    "        output_file_path = os.path.join(output_dir, f'{station_id}_sc_earth_with_wind.csv')\n",
    "        \n",
    "        # Load the CSV files\n",
    "        daily_df = pd.read_csv(daily_file_path)\n",
    "        station_df = pd.read_csv(station_file_path)\n",
    "\n",
    "        # Ensure the 'time' columns are in the same format\n",
    "        daily_df['time'] = pd.to_datetime(daily_df['time'])\n",
    "        station_df['time'] = pd.to_datetime(station_df['time'])\n",
    "\n",
    "        # Merge the dataframes on the 'time' column\n",
    "        merged_df = pd.merge(station_df, daily_df[['time', 'windspd']], on='time', how='left')\n",
    "\n",
    "        # Rename the column to 'reanalysis_wind'\n",
    "        merged_df.rename(columns={'windspd': 'reanalysis_wind'}, inplace=True)\n",
    "\n",
    "        # Save the merged DataFrame to a new CSV file\n",
    "        merged_df.to_csv(output_file_path, index=False)\n",
    "        print(f\"Saved merged data to {output_file_path}\")\n",
    "\n",
    "merge_daily_wind_reanalysis_to_sc_earth_files(reanalysis_station_path, sc_earth_path,interim_station_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3050519_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3050519_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3050519_UTF_hly_prec.csv\n",
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3030768_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3030768_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3030768_UTF_hly_prec.csv\n",
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3050778_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3050778_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3050778_UTF_hly_prec.csv\n",
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3044533_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3044533_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3044533_UTF_hly_prec.csv\n",
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3011887_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3011887_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3011887_UTF_hly_prec.csv\n",
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3053536_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3053536_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3053536_UTF_hly_prec.csv\n",
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3035208_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3035208_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3035208_UTF_hly_prec.csv\n",
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/1145M29_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/1145M29_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/1145M29_UTF_hly_prec.csv\n",
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/1176755_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/1176755_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/1176755_UTF_hly_prec.csv\n",
      "Daily averages have been written to /Users/dcasson/Data/pems/smith_2019/daily_data/3015523_UTF_hly_prec.csv\n",
      "Converted and copied: /Users/dcasson/Data/pems/smith_2019/hourly_data/3015523_UTF_hly_prec.txt to /Users/dcasson/Data/pems/smith_2019/daily_data/3015523_UTF_hly_prec.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/BANFFCS.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/BOWISLAND.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/BOWVALLEY.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/MILKRIVER.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/CORONATIONCLIMATE.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/JASPERWARDEN.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/PINCHERCREEKCLIMATE.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/NELSONCS.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/REVELSTOKEAIRPORTAUTO.csv\n",
      "Merged data saved to /Users/dcasson/Data/pems/undercatch/merged_stations/ROCKYMTNHOUSE(AUT).csv\n"
     ]
    }
   ],
   "source": [
    "def convert_txt_to_csv(txt_file, csv_file):\n",
    "    # Read the .txt file\n",
    "    with open(txt_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Assume the first non-empty line is the header and skip the second header line\n",
    "    header = next(line for line in lines if line.strip())\n",
    "    lines = lines[lines.index(header) + 2:]  # Skip the header line and the next line\n",
    "\n",
    "    # Write the content to a .csv file with the extracted header\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header.split())\n",
    "\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                writer.writerow(line.split())\n",
    "\n",
    "def calculate_daily_averages_from_eccc_paper(hourly_data_file, daily_data_file):\n",
    "\n",
    "    # Define columns\n",
    "    columns = [\"YYYYMMDDThhmm\", \"Unadj_P(mm)\", \"Tair(C)\", \"Wind(m/s)\", \"Wind_Flag\", \n",
    "               \"CE\", \"UTF_Adj_P(mm)\", \"CODECON(mm)\", \"UTF_Adj+CODECON_P(mm)\", \"Adj_Flag\"]\n",
    "    \n",
    "    # Read the data file\n",
    "    df = pd.read_csv(hourly_data_file, delim_whitespace=True, skiprows=2, names=columns)\n",
    "    \n",
    "    # Replace -99999 with NaN\n",
    "    df.replace(-99999, np.nan, inplace=True)\n",
    "\n",
    "    # Read date column to datetime\n",
    "    df['YYYYMMDDThhmm'] = pd.to_datetime(df['YYYYMMDDThhmm'], format='%Y%m%dT%H%M')\n",
    "\n",
    "    # Shift the date column back by 1 hour\n",
    "    df['YYYYMMDDThhmm'] = df['YYYYMMDDThhmm'] + pd.Timedelta(hours=1)\n",
    "\n",
    "    # Write YYYYMMDD to Date column\n",
    "    df['Date'] = df['YYYYMMDDThhmm'].dt.strftime('%Y%m%d')\n",
    "    \n",
    "    # Extract date part from datetime\n",
    "    #df['Date'] = df['YYYYMMDDThhmm'].str[:8]\n",
    "    \n",
    "    # Calculate daily averages, ignoring flags\n",
    "    daily_avg = df.groupby('Date').agg({\n",
    "        \"Unadj_P(mm)\": \"sum\",\n",
    "        \"Tair(C)\": \"mean\",\n",
    "        \"Wind(m/s)\": \"mean\",\n",
    "        \"CE\": \"mean\",\n",
    "        \"UTF_Adj_P(mm)\": \"sum\",\n",
    "        \"CODECON(mm)\": \"mean\",\n",
    "        \"UTF_Adj+CODECON_P(mm)\": \"sum\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Write daily averages to a CSV file\n",
    "    daily_avg.to_csv(daily_data_file, index=False)\n",
    "    print(f\"Daily averages have been written to {daily_data_file}\")\n",
    "\n",
    "\n",
    "def read_station_id_list(csv_file, column_name):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Return a list of the entries in the specified column\n",
    "    return df[column_name].tolist()\n",
    "\n",
    "def select_and_convert_raw_data_stations(station_id_list,hourly_path, daily_path):\n",
    "    for station in station_id_list:\n",
    "        station = str(station)\n",
    "        for file_name in os.listdir(hourly_path):\n",
    "            if station in file_name and file_name.endswith('.txt'):\n",
    "                # Construct the full file paths\n",
    "                hourly_file = os.path.join(hourly_path, file_name)\n",
    "                daily_file = os.path.join(daily_path, file_name)\n",
    "\n",
    "                #Update .txt to .csv\n",
    "                daily_file = daily_file.replace('.txt', '.csv')\n",
    "\n",
    "                # Define the destination .csv file name\n",
    "                calculate_daily_averages_from_eccc_paper(hourly_file, daily_file)\n",
    "                print(f\"Converted and copied: {hourly_file} to {daily_file}\")\n",
    "\n",
    "\n",
    "def merge_obs_and_model_csv_files(station_id_list, smith_2019_daily_path, gsod_path, merged_station_path,station_name_list):\n",
    "\n",
    "    for station, name in zip(station_id_list,station_name_list):\n",
    "        station = str(station)\n",
    "        for file_name in os.listdir(smith_2019_daily_path):\n",
    "            if station in file_name and file_name.endswith('.csv'):\n",
    "                # Construct the full file paths\n",
    "                obs_file = os.path.join(smith_2019_daily_path, file_name)\n",
    "                model_file = os.path.join(gsod_path, f'station_{station}_sc_earth_with_wind.csv')\n",
    "                df1 = pd.read_csv(obs_file)\n",
    "                df2 = pd.read_csv(model_file)\n",
    "\n",
    "                # Rename the date columns to a common name\n",
    "                df1 = df1.rename(columns={'Date': 'date'})\n",
    "                df2 = df2.rename(columns={'time': 'date'})\n",
    "\n",
    "                # Strip any leading/trailing whitespace from the date column in df1\n",
    "                df1['date'] = df1['date'].astype(str).str.strip()\n",
    "                \n",
    "                # Convert the 'YYYYMMDD' date format in df1 to a proper datetime format\n",
    "                df1['date'] = pd.to_datetime(df1['date'])\n",
    "                \n",
    "                # Convert the date column in df2 to datetime format\n",
    "                df2['date'] = pd.to_datetime(df2['date'], format='%Y-%m-%d')\n",
    "                \n",
    "                # Convert all other columns to numeric, coercing errors to NaN\n",
    "                for col in df1.columns:\n",
    "                    if col != 'date':\n",
    "                        df1[col] = pd.to_numeric(df1[col], errors='coerce')\n",
    "                \n",
    "                for col in df2.columns:\n",
    "                    if col != 'date':\n",
    "                        df2[col] = pd.to_numeric(df2[col], errors='coerce')\n",
    "                \n",
    "                # Merge the dataframes on the 'date' column\n",
    "                merged_df = pd.merge(df1, df2, on='date', how='outer')\n",
    "                \n",
    "                # Drop rows where there are NaN values in any of the columns\n",
    "                merged_df = merged_df.dropna()\n",
    "                \n",
    "                # Save the merged DataFrame to a new CSV file\n",
    "                merged_df.to_csv(Path(merged_station_path, f'{name}.csv'), index=False)\n",
    "                print(f\"Merged data saved to {merged_station_path}/{name}.csv\")\n",
    "\n",
    "station_id_list = read_station_id_list(evaluation_stations_csv, 'Climate ID')\n",
    "station_name_list = read_station_id_list(evaluation_stations_csv, 'Name')\n",
    "station_name_list = [s.replace(' ', '') for s in station_name_list]\n",
    "\n",
    "\n",
    "select_and_convert_raw_data_stations(station_id_list, smith_2019_hourly_path, smith_2019_daily_path)\n",
    "merge_obs_and_model_csv_files(station_id_list, smith_2019_daily_path, interim_station_path, merged_station_path, station_name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undercatch calculated for /Users/dcasson/Data/pems/undercatch/merged_stations/PINCHERCREEKCLIMATE.csv\n",
      "Undercatch calculated for /Users/dcasson/Data/pems/undercatch/merged_stations/BOWISLAND.csv\n",
      "Undercatch calculated for /Users/dcasson/Data/pems/undercatch/merged_stations/NELSONCS.csv\n",
      "Undercatch calculated for /Users/dcasson/Data/pems/undercatch/merged_stations/BANFFCS.csv\n",
      "Undercatch calculated for /Users/dcasson/Data/pems/undercatch/merged_stations/CORONATIONCLIMATE.csv\n",
      "Undercatch calculated for /Users/dcasson/Data/pems/undercatch/merged_stations/ROCKYMTNHOUSE(AUT).csv\n",
      "Undercatch calculated for /Users/dcasson/Data/pems/undercatch/merged_stations/MILKRIVER.csv\n",
      "Undercatch calculated for /Users/dcasson/Data/pems/undercatch/merged_stations/BOWVALLEY.csv\n",
      "Undercatch calculated for /Users/dcasson/Data/pems/undercatch/merged_stations/REVELSTOKEAIRPORTAUTO.csv\n",
      "Undercatch calculated for /Users/dcasson/Data/pems/undercatch/merged_stations/JASPERWARDEN.csv\n"
     ]
    }
   ],
   "source": [
    "def calculate_undercatch_for_gsod_stations(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Ensure required columns are present\n",
    "            if all(col in df.columns for col in ['wind', 'tmean', 'prcp']):\n",
    "                # Calculate CE and corrected precipitation\n",
    "                df['CE'] = df.apply(lambda row: calculate_CE(row['wind'], row['tmean']), axis=1)\n",
    "                df['corrected_prcp'] = df.apply(lambda row: apply_undercatch(row['prcp'], row['CE']), axis=1)\n",
    "                #Update so that if corrected_prcp is 0, set CE to 1\n",
    "                #df.loc[df['corrected_prcp'] == 0, 'CE'] = 1\n",
    "                \n",
    "                # Save the updated DataFrame back to the CSV file\n",
    "                df.to_csv(file_path, index=False)\n",
    "                print(f\"Undercatch calculated for {file_path}\")\n",
    "\n",
    "# Process the CSV files in the directory\n",
    "#calculate_undercatch_for_gsod_stations(output_station_path)\n",
    "\n",
    "def calculate_undercatch_for_gsod_stations_with_reanalysis(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Ensure required columns are present\n",
    "            if all(col in df.columns for col in ['wind', 'tmean', 'prcp','reanalysis_wind']):\n",
    "                # Calculate CE and corrected precipitation\n",
    "                df['CE_sc_earth'] = df.apply(lambda row: calculate_CE(row['wind'], row['tmean']), axis=1)\n",
    "                df['corrected_prcp_sc_earth'] = df.apply(lambda row: apply_undercatch(row['prcp'], row['CE_sc_earth']), axis=1)\n",
    "\n",
    "                df['CE_sc_reanalysis'] = df.apply(lambda row: calculate_CE(row['reanalysis_wind'], row['tmean']), axis=1)\n",
    "                df['corrected_prcp_sc_reanalysis'] = df.apply(lambda row: apply_undercatch(row['prcp'], row['CE_sc_reanalysis']), axis=1)\n",
    "\n",
    "                df['CE_eccc_reanalysis'] = df.apply(lambda row: calculate_CE(row['reanalysis_wind'], row['Tair(C)']), axis=1)\n",
    "                df['corrected_prcp_eccc_reanalysis'] = df.apply(lambda row: apply_undercatch(row['Unadj_P(mm)'], row['CE_eccc_reanalysis']), axis=1)\n",
    "\n",
    "\n",
    "                # Save the updated DataFrame back to the CSV file\n",
    "                df.to_csv(file_path, index=False)\n",
    "                print(f\"Undercatch calculated for {file_path}\")\n",
    "\n",
    "# Process the CSV files in the directory\n",
    "calculate_undercatch_for_gsod_stations_with_reanalysis(merged_station_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/BOWISLAND_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/MILKRIVER_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/CORONATIONCLIMATE_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/NELSONCS_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_accumulated_precipitation.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_accumulated_precipitation.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def generate_accumulated_precipitation_plots_per_water_year(input_csv_path,station,output_dir):\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    \n",
    "    # Convert date column to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Add a water year column\n",
    "    df['water_year'] = df['date'].apply(lambda x: x.year if x.month < 10 else x.year + 1)\n",
    "    \n",
    "    # Get unique water years\n",
    "    water_years = df['water_year'].unique()\n",
    "    \n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    num_plots = len(water_years)\n",
    "    num_cols = math.ceil(math.sqrt(num_plots))\n",
    "    num_rows = math.ceil(num_plots / num_cols)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(24, 6 * num_rows), sharex=False)\n",
    "\n",
    "    # Flatten axes array for easy iteration\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # Calculate accumulated precipitation for each water year and plot\n",
    "    for ax, water_year in zip(axes, water_years):\n",
    "        group = df[df['water_year'] == water_year].sort_values(by='date')\n",
    "        group['Accum_Unadj_P(mm)'] = group['Unadj_P(mm)'].cumsum()\n",
    "        group['Accum_prcp'] = group['prcp'].cumsum()\n",
    "        group['Accum_UTF_Adj_P(mm)'] = group['UTF_Adj_P(mm)'].cumsum()\n",
    "        group['Accum_corrected_prcp_sc_earth'] = group['corrected_prcp_sc_earth'].cumsum()\n",
    "        group['Accum_corrected_prcp_sc_reanalysis'] = group['corrected_prcp_sc_reanalysis'].cumsum()\n",
    "        group['Accum_corrected_prcp_eccc_reanalysis'] = group['corrected_prcp_eccc_reanalysis'].cumsum()\n",
    "\n",
    "        ax.plot(group['date'], group['Accum_Unadj_P(mm)'], label='Raw Gauge (ECCC)', linestyle='-', color='red')\n",
    "        ax.plot(group['date'], group['Accum_prcp'], label='Raw Gauge (SC-Earth)', linestyle='-', color='blue')\n",
    "        ax.plot(group['date'], group['Accum_UTF_Adj_P(mm)'], label='Undercatch Corrected (ECCC)', linestyle=':', color='red')\n",
    "        ax.plot(group['date'], group['Accum_corrected_prcp_sc_earth'], label='Undercatch Corrected (SC-Earth)', linestyle=':', color='blue')\n",
    "        ax.plot(group['date'], group['Accum_corrected_prcp_sc_reanalysis'], label='Undercatch Corrected (SC-Earth+ERA5 Wind)', linestyle=':', color='purple')\n",
    "        ax.plot(group['date'], group['Accum_corrected_prcp_eccc_reanalysis'], label='Undercatch Corrected (ECCC+ERA5 Wind)', linestyle=':', color='green')\n",
    "        ax.set_xlabel('Month')\n",
    "        ax.set_ylabel('Accumulated Precipitation (mm)')\n",
    "        ax.set_title(f'Accumulated Precipitation (Water Year {water_year})')\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Format x-axis to display only the month\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "\n",
    "        # Maintain the right x-axis bound\n",
    "        ax.set_xlim([group['date'].min(), group['date'].max()])\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_plots, num_rows * num_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    # Add a single legend at the bottom of the figure\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=3)\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 1])  # Adjust rect to make space for the legend\n",
    "\n",
    "    # Save the combined plot to a PNG file\n",
    "    output_plot_path = os.path.join(output_dir, f'{station}_accumulated_precipitation.png')\n",
    "    print(f\"Saving plot to {output_plot_path}\")\n",
    "    plt.savefig(output_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "# Run the function to generate the plots\n",
    "\n",
    "for station in station_name_list:\n",
    "    station = str(station)\n",
    "    comparative_file = Path(merged_station_path,f'{station}.csv')\n",
    "\n",
    "    generate_accumulated_precipitation_plots_per_water_year(comparative_file,station,plot_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/BOWISLAND_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/MILKRIVER_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/CORONATIONCLIMATE_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/NELSONCS_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_CE.png\n",
      "Saving plot to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_CE.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_ce_boxplot(file_path,station,output_dir,legend_title='Method', method_labels=None):\n",
    "    \"\"\"\n",
    "    Plots a boxplot of CE for each method aggregated by month.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the CSV file\n",
    "    - legend_title: str, title for the legend\n",
    "    - method_labels: dict, dictionary to map method column names to desired labels\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert the date column to datetime format\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "    # Extract the month from the date column\n",
    "    data['month'] = data['date'].dt.month\n",
    "\n",
    "    # Melt the dataset to have one column for method and another for CE values\n",
    "    melted_data = pd.melt(data, id_vars=['month'], value_vars=['CE', 'CE_sc_earth','CE_sc_reanalysis','CE_eccc_reanalysis'], var_name='method', value_name='CE_calc')\n",
    "    #Drop all values where CE is 1\n",
    "    melted_data = melted_data[melted_data['CE_calc'] != 1]\n",
    "\n",
    "    # Apply method labels if provided\n",
    "    if method_labels:\n",
    "        melted_data['method'] = melted_data['method'].map(method_labels)\n",
    "\n",
    "    # Create the boxplot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='month', y='CE_calc', hue='method', data=melted_data)\n",
    "    plt.title(f'Boxplot of CE for {station}')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('CE')\n",
    "    plt.legend(title=legend_title)\n",
    "    plt.grid(True)\n",
    "    # Save the plot to a PNG file\n",
    "    output_plot_path = f'{output_dir}/{station}_CE.png'\n",
    "    print(f'Saving plot to {output_plot_path}')\n",
    "    plt.savefig(output_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "# Usage example\n",
    "for station in station_name_list:\n",
    "    station = str(station)\n",
    "    comparative_file = Path(merged_station_path,f'{station}.csv')\n",
    "    \n",
    "    method_labels = {'CE': 'ECCC Analysis', 'CE_sc_earth': 'SC-Earth','CE_eccc_reanalysis': 'ECCC-ERA5','CE_sc_reanalysis': 'SC-Earth-ERA5'}\n",
    "    plot_ce_boxplot(comparative_file,station,plot_path,legend_title='CE Methods', method_labels=method_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWISLAND_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWISLAND_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWISLAND_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/MILKRIVER_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/MILKRIVER_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/MILKRIVER_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/CORONATIONCLIMATE_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/CORONATIONCLIMATE_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/CORONATIONCLIMATE_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/NELSONCS_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/NELSONCS_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/NELSONCS_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_reanalysis_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_wind.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_tmean.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_reanalysis_wind.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def generate_comparative_plots(file_path, col1, col2, output_dir, station_name, x_label=None, y_label=None):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Plot correlation with regression line\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(x=col1, y=col2, data=df, line_kws={\"color\": \"red\", \"alpha\": 0.7, \"lw\": 2})\n",
    "    \n",
    "    # Calculate correlation coefficient and regression equation\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(df[col1], df[col2])\n",
    "    regression_formula = f'Y = {intercept:.2f} + {slope:.2f}X'\n",
    "    correlation_coefficient = f'R^2 = {r_value**2:.2f}'\n",
    "    \n",
    "        # Add a 1:1 line for reference\n",
    "    max_val = max(df[col1].max(), df[col2].max())\n",
    "    min_val = min(df[col1].min(), df[col2].min())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='blue', linestyle='--', linewidth=1, label='1:1 Line')\n",
    "    \n",
    "    # Annotate plot with regression equation and R^2\n",
    "    plt.text(0.05, 0.95, regression_formula, transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "    plt.text(0.05, 0.90, correlation_coefficient, transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')\n",
    "    \n",
    "    # Set labels\n",
    "    plt.title(f'Correlation between {col1} and {col2}')\n",
    "    plt.xlabel(x_label if x_label else col1)\n",
    "    plt.ylabel(y_label if y_label else col2)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Define the output file path\n",
    "    output_file = os.path.join(output_dir, f'{station_name}_{col2}.png')\n",
    "    \n",
    "    # Save the plot to the specified directory\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {output_file}\")\n",
    "\n",
    "for station in station_name_list:\n",
    "    station = str(station)\n",
    "    comparative_file = Path(merged_station_path,f'{station}.csv')\n",
    "    generate_comparative_plots(comparative_file, 'Wind(m/s)', 'wind',plot_path, station, x_label='ECCC Wind', y_label='EC-Earth Wind')\n",
    "    generate_comparative_plots(comparative_file, 'Tair(C)', 'tmean', plot_path, station, x_label='ECCC Temperature (°C)', y_label='EC-Earth Temperature (°C)')\n",
    "    generate_comparative_plots(comparative_file, 'Wind(m/s)', 'reanalysis_wind',plot_path, station, x_label='ECCC Wind', y_label='reanalysis Wind')\n",
    "    #generate_comparative_plots(comparative_file, 'Tair(C)', 'tmean', plot_path, station, x_label='ECCC Temperature (°C)', y_label='EC-Earth Temperature (°C)')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_wind_distribution.png\n"
     ]
    }
   ],
   "source": [
    "def plot_histogram(df, columns,labels,plot_path,station_name, bins=30):\n",
    "    \"\"\"\n",
    "    Plots overlapping histograms for the specified columns in the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    columns (list): List of columns to plot.\n",
    "    bins (int): Number of bins for the histogram.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for column,label in zip(columns, labels):\n",
    "        plt.hist(df[column], bins=bins, alpha=0.5, label=label)\n",
    "    \n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Station {station_name} Wind Distribution')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    #plt.show()\n",
    "    # Define the output file path\n",
    "    output_file = os.path.join(plot_path, f'{station_name}_wind_distribution.png')\n",
    "    \n",
    "    # Save the plot to the specified directory\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {output_file}\")\n",
    "\n",
    "# Plot overlapping histograms for 'Wind(m/s)', 'wind', and 'reanalysis_wind'\n",
    "columns_to_plot = ['Wind(m/s)', 'wind', 'reanalysis_wind']\n",
    "labels = ['ECCC Wind', 'EC-Earth Wind', 'Reanalysis Wind']\n",
    "for station in station_name_list:\n",
    "    station = str(station)\n",
    "    comparative_file = Path(merged_station_path,f'{station}.csv')\n",
    "    df = pd.read_csv(comparative_file)\n",
    "    plot_histogram(df, columns_to_plot,labels, plot_path, station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BANFFCS_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/BOWVALLEY_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/JASPERWARDEN_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/PINCHERCREEKCLIMATE_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/REVELSTOKEAIRPORTAUTO_wind_distribution.png\n",
      "Plot saved to /Users/dcasson/Data/pems/undercatch/plots/ROCKYMTNHOUSE(AUT)_wind_distribution.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_histogram(df, columns, labels, plot_path, station_name, bins):\n",
    "    \"\"\"\n",
    "    Plots overlapping histograms for the specified columns in the dataframe using the provided bins.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    columns (list): List of columns to plot.\n",
    "    bins (array): Array of bin edges for the histogram.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for column, label in zip(columns, labels):\n",
    "        plt.hist(df[column], bins=bins, alpha=0.5, label=label)\n",
    "    \n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Station {station_name} Wind Distribution')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Define the output file path\n",
    "    output_file = os.path.join(plot_path, f'{station_name}_wind_distribution.png')\n",
    "    \n",
    "    # Save the plot to the specified directory\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {output_file}\")\n",
    "\n",
    "def determine_common_bins(df, columns, bins=30):\n",
    "    \"\"\"\n",
    "    Determines common bin edges for all columns in the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    columns (list): List of columns to consider for bin edges.\n",
    "    bins (int): Number of bins for the histogram.\n",
    "    \n",
    "    Returns:\n",
    "    array: Array of bin edges.\n",
    "    \"\"\"\n",
    "    min_value = df[columns].min().min()\n",
    "    max_value = df[columns].max().max()\n",
    "    bin_edges = np.linspace(min_value, max_value, bins + 1)\n",
    "    return bin_edges\n",
    "\n",
    "station_name_list = read_station_id_list(evaluation_stations_csv, 'Name')\n",
    "station_name_list = [s.replace(' ', '') for s in station_name_list]\n",
    "columns_to_plot = ['Wind(m/s)', 'wind', 'reanalysis_wind']\n",
    "labels = ['ECCC Wind', 'EC-Earth Wind', 'Reanalysis Wind']\n",
    "\n",
    "# Plot histograms for each station\n",
    "for station in station_name_list:\n",
    "    station = str(station)\n",
    "    comparative_file = Path(merged_station_path, f'{station}.csv')\n",
    "    df = pd.read_csv(comparative_file)\n",
    "    common_bins = determine_common_bins(df, columns_to_plot)\n",
    "    plot_histogram(df, columns_to_plot, labels, plot_path, station, common_bins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpep_snakemake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
