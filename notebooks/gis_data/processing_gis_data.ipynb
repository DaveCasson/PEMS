{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissolve_to_external_boundary(gpkg_path, layer_name=None, output_path=None, min_area=0):\n",
    "    \"\"\"\n",
    "    Dissolves all interior polygons from a GeoPackage layer, leaving only the external boundary.\n",
    "    \n",
    "    Parameters:\n",
    "    - gpkg_path (str): Path to the input GeoPackage file.\n",
    "    - layer_name (str, optional): Name of the layer to load from the GeoPackage. If None, loads the first layer.\n",
    "    - output_path (str, optional): Path to save the dissolved GeoPackage. If None, does not save the output.\n",
    "    - min_area (float, optional): Minimum area threshold to filter out small artefacts. Default is 0.\n",
    "    \n",
    "    Returns:\n",
    "    - result_gdf (GeoDataFrame): GeoDataFrame containing the dissolved external boundary.\n",
    "    \"\"\"\n",
    "    # Load the specified layer from the GeoPackage\n",
    "    gdf = gpd.read_file(gpkg_path, layer=layer_name)\n",
    "\n",
    "    # Ensure geometries are valid\n",
    "    gdf['geometry'] = gdf['geometry'].buffer(0)\n",
    "\n",
    "    # Union all geometries into a single geometry\n",
    "    unioned = unary_union(gdf.geometry)\n",
    "\n",
    "    # Function to remove interiors from Polygons\n",
    "    def remove_interiors(geom):\n",
    "        if geom.geom_type == 'Polygon':\n",
    "            return Polygon(geom.exterior)\n",
    "        elif geom.geom_type == 'MultiPolygon':\n",
    "            return MultiPolygon([Polygon(p.exterior) for p in geom.geoms])\n",
    "        else:\n",
    "            return geom  # Return geometry as is if it's not a polygon\n",
    "\n",
    "    # Remove interiors to eliminate holes\n",
    "    external_boundary = remove_interiors(unioned)\n",
    "\n",
    "    # Filter out small artefacts based on the min_area threshold\n",
    "    if external_boundary.geom_type == 'MultiPolygon':\n",
    "        filtered_polygons = [p for p in external_boundary.geoms if p.area >= min_area]\n",
    "        external_boundary = MultiPolygon(filtered_polygons)\n",
    "    elif external_boundary.geom_type == 'Polygon':\n",
    "        if external_boundary.area < min_area:\n",
    "            external_boundary = None  # Discard if below min_area\n",
    "\n",
    "    # Create a GeoDataFrame from the external boundary\n",
    "    result_gdf = gpd.GeoDataFrame(geometry=[external_boundary], crs=gdf.crs)\n",
    "\n",
    "    # Optionally save the result to a new GeoPackage\n",
    "    if output_path and external_boundary is not None:\n",
    "        result_gdf.to_file(output_path, driver=\"GPKG\")\n",
    "    \n",
    "    return result_gdf\n",
    "\n",
    "# Example usage\n",
    "input = '/Users/dcasson/GitHub/PEMS/gis_data/tuolumne/tuolumne_tdx.gpkg'\n",
    "output = '/Users/dcasson/GitHub/PEMS/gis_data/tuolumne/tuolumne_boundary.gpkg'\n",
    "\n",
    "dissolved_boundary = dissolve_to_external_boundary(input, output_path=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved shapefile: tuolumne_dem_bbox.shp\n",
      "Saved shapefile: tuolumne_station_bbox.shp\n",
      "Saved shapefile: tuolumne_station_eval_bbox.shp\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "def create_bboxes_shapefiles(\n",
    "    yaml_path, \n",
    "    prefix=\"bbox\", \n",
    "    output_dir=\".\",\n",
    "    bbox_keys=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads bounding boxes from a YAML configuration and writes each one \n",
    "    to a separate shapefile.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    yaml_path : str\n",
    "        Path to the YAML file containing bounding boxes and other configs.\n",
    "    prefix : str, optional\n",
    "        A filename prefix for the output shapefiles, e.g. \"myprefix\". \n",
    "        Defaults to \"bbox\".\n",
    "    output_dir : str, optional \n",
    "    bbox_keys : list of str, optional\n",
    "        The keys in the YAML file that define bounding boxes. \n",
    "        Defaults to [\"dem_bbox\", \"station_bbox\", \"station_eval_bbox\"].\n",
    "    \"\"\"\n",
    "    # Default keys if none are provided\n",
    "    if bbox_keys is None:\n",
    "        bbox_keys = [\"dem_bbox\", \"station_bbox\", \"station_eval_bbox\"]\n",
    "    \n",
    "    # 1. Read YAML\n",
    "    with open(yaml_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # 2. For each bounding box key, create a shapefile\n",
    "    for key in bbox_keys:\n",
    "        # Skip if not present in config\n",
    "        if key not in config:\n",
    "            print(f\"Warning: '{key}' not found in YAML.\")\n",
    "            continue\n",
    "        \n",
    "        # 3. Extract the bounding box coords\n",
    "        coords = config[key]\n",
    "        lon_min = coords[\"lon_min\"]\n",
    "        lat_min = coords[\"lat_min\"]\n",
    "        lon_max = coords[\"lon_max\"]\n",
    "        lat_max = coords[\"lat_max\"]\n",
    "        \n",
    "        # 4. Create a single Polygon (Shapely box)\n",
    "        polygon = box(lon_min, lat_min, lon_max, lat_max)\n",
    "        \n",
    "        # 5. Wrap in a GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame({\"name\": [key]}, \n",
    "                               geometry=[polygon], \n",
    "                               crs=\"EPSG:4326\")\n",
    "        \n",
    "        # 6. Construct filename with user-defined prefix\n",
    "        shapefile_name = f\"{prefix}_{key}.shp\"\n",
    "        shapefile_path = os.path.join(output_dir, shapefile_name)\n",
    "        # 7. Write to disk\n",
    "        gdf.to_file(shapefile_path, driver=\"ESRI Shapefile\")\n",
    "        print(f\"Saved shapefile: {shapefile_name}\")\n",
    "\n",
    "catchment = 'tuolumne'\n",
    "yaml_file = f\"/Users/dcasson/GitHub/gpep_snakemake/workflow/config/gpep_data_prep_config_{catchment}.yaml\"\n",
    "output_dir = f'/Users/dcasson/GitHub/PEMS/gis_data/{catchment}'\n",
    " # path to your YAML file\n",
    "create_bboxes_shapefiles(\n",
    "    yaml_path=yaml_file,\n",
    "    prefix=catchment, \n",
    "    output_dir=output_dir,\n",
    "    bbox_keys=[\"dem_bbox\", \"station_bbox\", \"station_eval_bbox\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick compare of two netcdf files\n",
    "import xarray as xr\n",
    "\n",
    "variable = 'scalarSWE'\n",
    "nc1 = '/Users/dcasson/Data/yukon_esp/ross_from_camille/model/output/summa_output/run1_day.nc'\n",
    "nc2 ='/Users/dcasson/Downloads/run1_day.nc'\n",
    "ds1 = xr.open_dataset(nc1)\n",
    "ds2 = xr.open_dataset(nc2)\n",
    "\n",
    "#Create a plot that shows both variables side by side\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summa_snakemake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
